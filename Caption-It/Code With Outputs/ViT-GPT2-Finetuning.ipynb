{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":114911314,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"tarun\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:28:42.335464Z","iopub.execute_input":"2025-04-27T15:28:42.335722Z","iopub.status.idle":"2025-04-27T15:28:42.478415Z","shell.execute_reply.started":"2025-04-27T15:28:42.335701Z","shell.execute_reply":"2025-04-27T15:28:42.477888Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip uninstall -y peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:28:45.315511Z","iopub.execute_input":"2025-04-27T15:28:45.315771Z","iopub.status.idle":"2025-04-27T15:28:47.465432Z","shell.execute_reply.started":"2025-04-27T15:28:45.315751Z","shell.execute_reply":"2025-04-27T15:28:47.464559Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: peft 0.14.0\nUninstalling peft-0.14.0:\n  Successfully uninstalled peft-0.14.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip uninstall -y transformers\n!pip install --no-cache-dir transformers==4.31.0 accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:28:50.788766Z","iopub.execute_input":"2025-04-27T15:28:50.789041Z","iopub.status.idle":"2025-04-27T15:30:01.767621Z","shell.execute_reply.started":"2025-04-27T15:28:50.789016Z","shell.execute_reply":"2025-04-27T15:30:01.766887Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.51.1\nUninstalling transformers-4.51.1:\n  Successfully uninstalled transformers-4.51.1\nCollecting transformers==4.31.0\n  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.31.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.31.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m133.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m341.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m282.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m137.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m152.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m166.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m167.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.13.3 transformers-4.31.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install --no-cache-dir -q \\\n      \"evaluate @ git+https://github.com/huggingface/evaluate.git\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T03:40:28.457560Z","iopub.execute_input":"2025-04-27T03:40:28.457856Z","iopub.status.idle":"2025-04-27T03:40:36.980354Z","shell.execute_reply.started":"2025-04-27T03:40:28.457832Z","shell.execute_reply":"2025-04-27T03:40:36.979681Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for evaluate (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ⬇︎ 1-A  UNINSTALL the old package, then\n# ⬇︎ 1-B  INSTALL the latest code straight from GitHub\n!pip uninstall -y evaluate\n!pip install --no-cache-dir -q \\\n      \"evaluate @ git+https://github.com/huggingface/evaluate.git\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:31:19.914008Z","iopub.execute_input":"2025-04-27T15:31:19.914812Z","iopub.status.idle":"2025-04-27T15:31:28.204628Z","shell.execute_reply.started":"2025-04-27T15:31:19.914775Z","shell.execute_reply":"2025-04-27T15:31:28.203898Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping evaluate as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for evaluate (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!mkdir -p local_metrics/cider\n!wget -q https://raw.githubusercontent.com/huggingface/evaluate/main/metrics/cider/cider.py \\\n         -O  local_metrics/cider/cider.py\n!wget -q https://raw.githubusercontent.com/huggingface/evaluate/main/metrics/cider/__init__.py \\\n         -O  local_metrics/cider/__init__.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:31:33.993693Z","iopub.execute_input":"2025-04-27T15:31:33.993973Z","iopub.status.idle":"2025-04-27T15:31:34.666766Z","shell.execute_reply.started":"2025-04-27T15:31:33.993948Z","shell.execute_reply":"2025-04-27T15:31:34.665955Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install -q --no-cache-dir pycocoevalcap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:31:38.609668Z","iopub.execute_input":"2025-04-27T15:31:38.610227Z","iopub.status.idle":"2025-04-27T15:31:42.807875Z","shell.execute_reply.started":"2025-04-27T15:31:38.610197Z","shell.execute_reply":"2025-04-27T15:31:42.806964Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m320.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from pycocoevalcap.cider.cider import Cider as CiderScorer\n\ndef cider_score(preds, refs):\n    \"\"\"\n    preds : List[str]            – one generated caption per image\n    refs  : List[List[str]]      – 5 (or more) ground-truth captions per image\n    returns a single float in [0, 100]\n    \"\"\"\n    scorer = CiderScorer()\n    gts, res = {}, {}\n\n    for i, (pred, ref_list) in enumerate(zip(preds, refs)):\n        res[i] = [pred]           # list *of strings* (hypotheses)\n        gts[i] = ref_list         # list *of strings* (references)\n\n    score, _ = scorer.compute_score(gts, res)\n    return score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:31:46.052496Z","iopub.execute_input":"2025-04-27T15:31:46.052964Z","iopub.status.idle":"2025-04-27T15:31:46.063815Z","shell.execute_reply.started":"2025-04-27T15:31:46.052933Z","shell.execute_reply":"2025-04-27T15:31:46.063232Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"p  = [\"a cat on a mat\"]\nrs = [[\"a cat is sitting on the mat\", \"cat on a mat\"]]\nprint(\"CIDEr:\", cider_score(p, rs))       # e.g. CIDEr: 41.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:31:49.845664Z","iopub.execute_input":"2025-04-27T15:31:49.846374Z","iopub.status.idle":"2025-04-27T15:31:49.850932Z","shell.execute_reply.started":"2025-04-27T15:31:49.846350Z","shell.execute_reply":"2025-04-27T15:31:49.850312Z"}},"outputs":[{"name":"stdout","text":"CIDEr: 0.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:31:54.217592Z","iopub.execute_input":"2025-04-27T15:31:54.217872Z","iopub.status.idle":"2025-04-27T15:31:58.973894Z","shell.execute_reply.started":"2025-04-27T15:31:54.217850Z","shell.execute_reply":"2025-04-27T15:31:58.973194Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b695efe123d1afcf70ebf25e164ae86162b94781c83d449b6780f5df32c73a76\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Without Fine tuning","metadata":{}},{"cell_type":"code","source":"#####################################################################\n# 0.  Runtime installs  (run FIRST, before any other import)\n#####################################################################\n# !pip install -q --no-cache-dir \\\n#       \"transformers==4.35.2\" \"accelerate>=0.25.0\" \"evaluate>=0.4.3\" \\\n#       \"sacrebleu\" \"rouge_score\"\n\n#####################################################################\n# 1.  Imports & basic setup\n#####################################################################\nimport os, random, ssl, math, numpy as np, json, torch\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom PIL import Image\nfrom io import BytesIO\nfrom tqdm.auto import tqdm\nfrom typing import List\n\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, random_split\nfrom torch import nn\n\nfrom transformers import (\n    VisionEncoderDecoderModel, ViTImageProcessor,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n    get_cosine_schedule_with_warmup,\n    default_data_collator, set_seed\n)\n# ---- internal GPT-2 building block\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2Block\n\n# from transformers import (\n#     VisionEncoderDecoderModel, ViTImageProcessor,\n#     AutoTokenizer, GPT2Block,\n#     Seq2SeqTrainingArguments, Seq2SeqTrainer,\n#     get_cosine_schedule_with_warmup,\n#     default_data_collator, set_seed\n# )\nfrom evaluate import load\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nset_seed(42)\nssl._create_default_https_context = ssl._create_unverified_context\n\n#####################################################################\n# 2.  Paths & hyper-parameters\n#####################################################################\nDATASET_DIR   = Path(\"/kaggle/input/flickr8k\")        # ← change if needed\nIMG_DIR       = DATASET_DIR / \"Images\"\nCAPTION_FILE  = DATASET_DIR / \"captions.txt\"\n\nENCODER_NAME  = \"google/vit-base-patch16-224-in21k\"\nDECODER_NAME  = \"gpt2\"\n\nEPOCHS                 = 10\nTRAIN_VAL_SPLIT        = 0.95\nPER_DEVICE_BATCH       = 2\nGRAD_ACCUM_STEPS       = 8           # effective 16\nLR_DECODER             = 3e-5        # a bit lower because we add fresh blocks\nLR_ENCODER             = 1e-5\nMAX_SEQ_LEN            = 24\nWARMUP_RATIO           = 0.05\nWEIGHT_DECAY           = 0.01\nLABEL_SMOOTH           = 0.1\nNEW_GPT2_BLOCKS        = 2           # ← extra capacity\nUNFREEZE_VIT_AFTER_EP  = 2\nUNFREEZE_VIT_LAST_N    = 2           # fine-tune last 2 encoder blocks\n\n#####################################################################\n# 3.  Caption loader (comma-separated Flickr8k)\n#####################################################################\nimport pandas as pd\ndef load_flickr(path: Path):\n    df = pd.read_csv(path)\n    buckets = defaultdict(list)\n    for img, cap in zip(df[\"image\"], df[\"caption\"]):\n        buckets[img.split(\"#\")[0]].append(cap.strip())\n    return list(buckets.items())\n\nitems = load_flickr(CAPTION_FILE)\nprint(\"Unique images:\", len(items))\n\n#####################################################################\n# 4.  Transforms, processor, tokenizer\n#####################################################################\nimage_processor = ViTImageProcessor.from_pretrained(ENCODER_NAME)\ntokenizer = AutoTokenizer.from_pretrained(DECODER_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\ntrain_tfm = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.9,1.0)),\n    transforms.RandomHorizontalFlip()\n])\nval_tfm = transforms.Resize((224,224))\n\n#####################################################################\n# 5.  Dataset\n#####################################################################\nclass ImgCapDS(Dataset):\n    def __init__(self, pairs, img_root, tfm):\n        self.pairs = pairs; self.root = img_root; self.tfm = tfm\n    def __len__(self): return len(self.pairs)\n    def __getitem__(self, idx):\n        img_name, caps = self.pairs[idx]\n        with Image.open(self.root/img_name).convert(\"RGB\") as im:\n            im = self.tfm(im)\n        px = image_processor(im, return_tensors=\"pt\").pixel_values.squeeze(0)\n        cap = random.choice(caps)\n        lab = tokenizer(cap, max_length=MAX_SEQ_LEN,\n                        padding=\"max_length\", truncation=True).input_ids\n        lab = [(x if x!=tokenizer.pad_token_id else -100) for x in lab]\n        return {\"pixel_values\": px,\n                \"labels\": torch.tensor(lab, dtype=torch.long),\n                \"all_caps\": caps}\n\nfull_train = ImgCapDS(items, IMG_DIR, train_tfm)\nfull_val   = ImgCapDS(items, IMG_DIR, val_tfm)\nn_train    = int(TRAIN_VAL_SPLIT*len(full_train))\ntrain_ds,_ = random_split(full_train,[n_train,len(full_train)-n_train])\nval_ds     = random_split(full_val  ,[n_train,len(full_val)-n_train])[1]\nprint(f\"train {len(train_ds)}  | val {len(val_ds)}\")\n\n# 6.  Build base ViT-GPT-2, then **extend** it\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n            ENCODER_NAME, DECODER_NAME, tie_encoder_decoder=False)\n\ndec_conf  = model.decoder.config\nenc_size  = model.encoder.config.hidden_size\ndec_size  = dec_conf.n_embd\n\n# 6-A  2-layer MLP between encoder and decoder\nmodel.enc_to_dec_proj = nn.Sequential(\n    nn.Linear(enc_size, dec_size),\n    nn.Tanh(),\n    nn.Dropout(0.1),\n    nn.Linear(dec_size, dec_size),\n).to(device)\n\n# 6-B  Append fresh GPT-2 blocks\nfor _ in range(NEW_GPT2_BLOCKS):\n    model.decoder.transformer.h.append(GPT2Block(dec_conf).to(device))\n\nnew_n = len(model.decoder.transformer.h)          # ← 14\ndec_conf.n_layer = new_n \nmodel.decoder.transformer.n_layer = new_n        # NEW: internal attribute\nmodel.decoder.transformer.config.n_layer = new_n # NEW: its own config ref\nmodel.decoder.config.n_layer = new_n\n\n# 6-C  misc config …\n\n# ---- 6-C  Misc config tweaks ----\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.eos_token_id           = tokenizer.eos_token_id\nmodel.config.pad_token_id           = tokenizer.pad_token_id\nmodel.config.max_length             = MAX_SEQ_LEN\nmodel.config.no_repeat_ngram_size   = 4\nmodel.config.length_penalty         = 1.2\nmodel.config.early_stopping         = True\nmodel.config.label_smoothing        = LABEL_SMOOTH\n\n# Freeze encoder at start\nfor p in model.encoder.parameters(): p.requires_grad = False\nmodel.to(device)\n\n#####################################################################\n# 7.  Metrics (BLEU & ROUGE-L)\n#####################################################################\nbleu = load(\"bleu\"); rouge = load(\"rouge\")\ndef _post(preds, labels):\n    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels!=-100, labels, tokenizer.pad_token_id)\n    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return [p.strip() for p in preds], [[l.strip()] for l in labels]\n\ndef compute_metrics(eval_pred):\n    p,l = _post(*eval_pred)\n    out = bleu.compute(predictions=p, references=l)\n    out[\"rougeL\"] = rouge.compute(predictions=p,\n                     references=[r[0] for r in l],\n                     rouge_types=[\"rougeL\"])[\"rougeL\"]\n    out[\"cider\"]  = cider_score(p, l)\n    return out\n\n#####################################################################\n# 8.  Custom Trainer: 2-group optimiser + gradual unfreeze\n#####################################################################\nclass VCaptionTrainer(Seq2SeqTrainer):\n    def _setup_training(self,*a,**k):\n        super()._setup_training(*a,**k)\n        enc,dec=[],[]\n        for n,p in self.model.named_parameters():\n            (enc if n.startswith(\"encoder.\") else dec).append(p)\n        self.optimizer = torch.optim.AdamW(\n            [{\"params\":enc,\"lr\":LR_ENCODER},\n             {\"params\":dec,\"lr\":LR_DECODER}], lr=LR_DECODER,\n            weight_decay=WEIGHT_DECAY)\n        total = self.args.max_steps\n        warm  = int(WARMUP_RATIO*total)\n        self.lr_scheduler = get_cosine_schedule_with_warmup(\n            self.optimizer, warm, total)\n    def training_step(self,*a,**k):\n        if self.state.epoch and self.state.epoch>=UNFREEZE_VIT_AFTER_EP:\n            for idx in range(12-UNFREEZE_VIT_LAST_N,12):\n                for p in self.model.encoder.encoder.layer[idx].parameters():\n                    p.requires_grad=True\n        return super().training_step(*a,**k)\n\n#####################################################################\n# 9.  TrainingArguments\n#####################################################################\nargs = Seq2SeqTrainingArguments(\n    output_dir=\"vit_gpt2_extended\",\n    per_device_train_batch_size=PER_DEVICE_BATCH,\n    per_device_eval_batch_size=PER_DEVICE_BATCH,\n    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n    num_train_epochs=EPOCHS,\n    predict_with_generate=True,\n    generation_max_length=MAX_SEQ_LEN,\n    generation_num_beams=4,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"cider\",\n    greater_is_better=True,\n    fp16=True,\n    logging_steps=50,\n    report_to=\"none\",\n)\n\ntrainer = VCaptionTrainer(\n    model=model, args=args,\n    train_dataset=train_ds, eval_dataset=val_ds,\n    tokenizer=image_processor,\n    data_collator=default_data_collator,\n    compute_metrics=compute_metrics)\n\n#####################################################################\n# 10.  Train & evaluate\n#####################################################################\ntrainer.train()\ntrainer.evaluate()\n\n# #####################################################################\n# # 11.  Inference helper  (unchanged API)\n# #####################################################################\n# import requests, textwrap\n# def caption_url(url, num_beams=4):\n#     model.eval()\n#     img = Image.open(BytesIO(requests.get(url,timeout=10).content)).convert(\"RGB\")\n#     px  = image_processor(img, return_tensors=\"pt\").pixel_values.to(device)\n#     with torch.no_grad():\n#         out = model.generate(px, max_length=MAX_SEQ_LEN, num_beams=num_beams,\n#                              eos_token_id=tokenizer.eos_token_id,\n#                              pad_token_id=tokenizer.pad_token_id)\n#     return textwrap.shorten(tokenizer.decode(out[0], skip_special_tokens=True), 120)\n\n# # caption_url(\"https://huggingface.co/datasets/nateraw/horses-or-humans/resolve/main/train/horse/horse01.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T03:45:29.559140Z","iopub.execute_input":"2025-04-27T03:45:29.559431Z","iopub.status.idle":"2025-04-27T05:11:08.146197Z","shell.execute_reply.started":"2025-04-27T03:45:29.559409Z","shell.execute_reply":"2025-04-27T05:11:08.145442Z"}},"outputs":[{"name":"stderr","text":"2025-04-27 03:45:39.656506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745725539.850448      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745725539.903688      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Unique images: 8091\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"136156be514b4856a2302ffbbc517920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5322501e8f446ab20cdbf45a72d55e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e7d970bc6564339996bf20d0363d721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f77bbf2f66d43c79e1871d917137c69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b386b23e854aaaaf9e85ca06d782e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd52d75e61884cd49df446daedd92865"}},"metadata":{}},{"name":"stdout","text":"train 7686  | val 405\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc6591fb2eb450db720f90136bf61f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7050b4d8f3d145408d5084c6a7c14596"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa513444351400184bfecb376e71f30"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.8.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.5.ln_cross_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.4.ln_cross_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.1.ln_cross_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.11.ln_cross_attn.weight', 'h.8.ln_cross_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.8.ln_cross_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.6.ln_cross_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.10.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.5.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.6.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.3.ln_cross_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.2.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.9.ln_cross_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.10.ln_cross_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.0.ln_cross_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.7.ln_cross_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.bias', 'h.1.ln_cross_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee9e3beb3cb4a648582379d827647ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77fce29738c94b5b9d2897199e0c4028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bfdc25bdc104273a02867c04f82075e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db6a4af5df44df7a077a626d1fb8f9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3321f225e04debbd5b6d69e060985b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3840/3840 1:22:28, Epoch 7/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Precisions</th>\n      <th>Brevity Penalty</th>\n      <th>Length Ratio</th>\n      <th>Translation Length</th>\n      <th>Reference Length</th>\n      <th>Rougel</th>\n      <th>Cider</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.848100</td>\n      <td>2.699446</td>\n      <td>0.033205</td>\n      <td>[0.19982595453062113, 0.04676832043695949, 0.017893355600620303, 0.0072699924793181245]</td>\n      <td>1.000000</td>\n      <td>1.965156</td>\n      <td>9193</td>\n      <td>4678</td>\n      <td>0.233937</td>\n      <td>0.157716</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.638800</td>\n      <td>2.500588</td>\n      <td>0.036753</td>\n      <td>[0.20729720925181888, 0.0528169014084507, 0.019883319442790807, 0.008381285964473356]</td>\n      <td>1.000000</td>\n      <td>1.972793</td>\n      <td>9209</td>\n      <td>4668</td>\n      <td>0.248284</td>\n      <td>0.207218</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.597500</td>\n      <td>2.543770</td>\n      <td>0.028674</td>\n      <td>[0.1949134199134199, 0.04470854555744199, 0.015183867141162514, 0.005109034267912773]</td>\n      <td>1.000000</td>\n      <td>1.996111</td>\n      <td>9240</td>\n      <td>4629</td>\n      <td>0.237741</td>\n      <td>0.155776</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.639700</td>\n      <td>2.562127</td>\n      <td>0.032096</td>\n      <td>[0.2045060658578856, 0.046788263283108644, 0.017098076466397532, 0.006486216789322689]</td>\n      <td>1.000000</td>\n      <td>1.895299</td>\n      <td>9232</td>\n      <td>4871</td>\n      <td>0.245902</td>\n      <td>0.180522</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.635000</td>\n      <td>2.536200</td>\n      <td>0.031799</td>\n      <td>[0.2091411242283115, 0.05018124150430449, 0.016621156357592308, 0.0058618109254178095]</td>\n      <td>1.000000</td>\n      <td>1.918745</td>\n      <td>9233</td>\n      <td>4812</td>\n      <td>0.251728</td>\n      <td>0.184182</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.583700</td>\n      <td>2.531303</td>\n      <td>0.034483</td>\n      <td>[0.20753899480069324, 0.05075337034099921, 0.01887912609831394, 0.007109891480603717]</td>\n      <td>1.000000</td>\n      <td>1.924937</td>\n      <td>9232</td>\n      <td>4796</td>\n      <td>0.251029</td>\n      <td>0.187008</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.582400</td>\n      <td>2.613423</td>\n      <td>0.031449</td>\n      <td>[0.2019501625135428, 0.050538243626062324, 0.017458432304038006, 0.005489706799750468]</td>\n      <td>1.000000</td>\n      <td>1.964248</td>\n      <td>9230</td>\n      <td>4699</td>\n      <td>0.248362</td>\n      <td>0.167334</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.590700</td>\n      <td>2.547881</td>\n      <td>0.036108</td>\n      <td>[0.21053772766695578, 0.055221680462637486, 0.019847872593296886, 0.007366712448495442]</td>\n      <td>1.000000</td>\n      <td>1.930515</td>\n      <td>9224</td>\n      <td>4778</td>\n      <td>0.256353</td>\n      <td>0.213324</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='203' max='203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [203/203 02:31]\n    </div>\n    "},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 2.5550448894500732,\n 'eval_bleu': 0.03435276787915125,\n 'eval_precisions': [0.2032740676496097,\n  0.049325320331103296,\n  0.018540527691942,\n  0.007491571981520789],\n 'eval_brevity_penalty': 1.0,\n 'eval_length_ratio': 1.9751605995717345,\n 'eval_translation_length': 9224,\n 'eval_reference_length': 4670,\n 'eval_rougeL': 0.2501312044806918,\n 'eval_cider': 0.1968407337684648,\n 'eval_runtime': 152.3178,\n 'eval_samples_per_second': 2.659,\n 'eval_steps_per_second': 1.333,\n 'epoch': 7.99}"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# 11.  Inference helper  (unchanged API)\n#####################################################################\nimport requests, textwrap\ndef caption_url(url, num_beams=4):\n    model.eval()\n    img = Image.open(BytesIO(requests.get(url,timeout=10).content)).convert(\"RGB\")\n    px  = image_processor(img, return_tensors=\"pt\").pixel_values.to(device)\n    with torch.no_grad():\n        out = model.generate(px, max_length=MAX_SEQ_LEN, num_beams=num_beams,\n                             eos_token_id=tokenizer.eos_token_id,\n                             pad_token_id=tokenizer.pad_token_id)\n    return textwrap.shorten(tokenizer.decode(out[0], skip_special_tokens=True), 120)\n\ncaption_url(\"https://images.pexels.com/photos/1054655/pexels-photo-1054655.jpeg?cs=srgb&dl=pexels-hsapir-1054655.jpg&fm=jpg\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T05:23:22.029924Z","iopub.execute_input":"2025-04-27T05:23:22.030468Z","iopub.status.idle":"2025-04-27T05:23:23.046944Z","shell.execute_reply.started":"2025-04-27T05:23:22.030443Z","shell.execute_reply":"2025-04-27T05:23:23.046339Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'A man in a blue shirt is standing on the shore of a lake.... a man in a white'"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Fine tuned version - VIT GPT 2","metadata":{}},{"cell_type":"code","source":"#####################################################################\n# 0.  One-time installs – run BEFORE any other import\n#####################################################################\n# !pip install -q --no-cache-dir \"transformers==4.35.2\" \"accelerate>=0.25.0\" \\\n#                            \"evaluate>=0.4.3\" sacrebleu rouge_score \\\n#                            pycocoevalcap\n\n#####################################################################\n# 1.  Imports & basic setup\n#####################################################################\nimport os, random, ssl, math, json, torch, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom typing import List\nfrom PIL import Image\nfrom io import BytesIO\nfrom tqdm.auto import tqdm\n\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, random_split, WeightedRandomSampler\nfrom torch import nn\n\nfrom transformers import (\n    VisionEncoderDecoderModel, ViTImageProcessor,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n    get_cosine_schedule_with_warmup,\n    default_data_collator, set_seed\n)\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2Block\n\nfrom evaluate import load\nfrom pycocoevalcap.cider.cider import Cider as CiderScorer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nset_seed(42)\nssl._create_default_https_context = ssl._create_unverified_context\n\n#####################################################################\n# 2.  Paths & hyper-parameters\n#####################################################################\nDATASET_DIR   = Path(\"/kaggle/input/flickr8k\")        # change if needed\nIMG_DIR       = DATASET_DIR / \"Images\"\nCAPTION_FILE  = DATASET_DIR / \"captions.txt\"\n\nENCODER_NAME  = \"google/vit-base-patch16-224-in21k\"\nDECODER_NAME  = \"gpt2\"\n\nEPOCHS                 = 8\nTRAIN_VAL_SPLIT        = 0.95\nPER_DEVICE_BATCH       = 2\nGRAD_ACCUM_STEPS       = 16      # effective 32\nLR_DECODER             = 2e-5\nLR_ENCODER             = 5e-6\nMAX_SEQ_LEN            = 20\nWARMUP_RATIO           = 0.1\nWEIGHT_DECAY           = 0.01\nLABEL_SMOOTH           = 0.1\nNEW_GPT2_BLOCKS        = 2\nUNFREEZE_VIT_AFTER_EP  = 2\nUNFREEZE_VIT_LAST_N    = 12      # unfreeze all blocks\n\n#####################################################################\n# 3.  Load captions & expand to 1-caption-per-row\n#####################################################################\ndf = pd.read_csv(CAPTION_FILE)\npairs = defaultdict(list)\nfor img, cap in zip(df[\"image\"], df[\"caption\"]):\n    pairs[img.split(\"#\")[0]].append(cap.strip())\n\nexpanded = [(img, cap) for img, caps in pairs.items() for cap in caps]\nprint(\"Unique images:\", len(pairs), \"  expanded rows:\", len(expanded))\n\n#####################################################################\n# 4.  Processor, tokenizer, transforms\n#####################################################################\nprocessor = ViTImageProcessor.from_pretrained(ENCODER_NAME)\ntokenizer = AutoTokenizer.from_pretrained(DECODER_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\ntrain_tf = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n    transforms.RandomHorizontalFlip()\n])\nval_tf = transforms.Resize((224, 224))\n\n#####################################################################\n# 5.  Dataset with length-aware weights\n#####################################################################\nclass FlickrDS(Dataset):\n    def __init__(self, rows, img_root, tfm):\n        self.rows, self.root, self.tfm = rows, img_root, tfm\n        self.weights = torch.tensor([min(1.0, 10/len(c.split())) for _, c in rows])\n\n    def __len__(self): return len(self.rows)\n\n    def __getitem__(self, idx):\n        img_name, cap = self.rows[idx]\n        with Image.open(self.root/img_name).convert(\"RGB\") as im:\n            im = self.tfm(im)\n        px = processor(im, return_tensors=\"pt\").pixel_values.squeeze(0)\n        lab = tokenizer(cap, max_length=MAX_SEQ_LEN,\n                        padding=\"max_length\", truncation=True).input_ids\n        lab = [(x if x!=tokenizer.pad_token_id else -100) for x in lab]\n        return {\"pixel_values\": px, \"labels\": torch.tensor(lab, dtype=torch.long),\n                \"img\": img_name}\n\nfull_ds   = FlickrDS(expanded, IMG_DIR, train_tf)\nval_full  = FlickrDS(expanded, IMG_DIR, val_tf)\n\nn_train   = int(TRAIN_VAL_SPLIT*len(full_ds))\ntrain_ds, _ = random_split(full_ds, [n_train, len(full_ds)-n_train])\nval_ds      = random_split(val_full, [n_train, len(full_ds)-n_train])[1]\nprint(f\"train {len(train_ds)} | val {len(val_ds)}\")\n\n#####################################################################\n# 6.  Build ViT-GPT-2 & extend decoder\n#####################################################################\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n            ENCODER_NAME, DECODER_NAME, tie_encoder_decoder=False)\n\n# 6-A  2-layer MLP bridge\nhid_e = model.encoder.config.hidden_size\nhid_d = model.decoder.config.n_embd\nmodel.enc_to_dec_proj = nn.Sequential(\n    nn.Linear(hid_e, hid_d),\n    nn.Tanh(),\n    nn.Dropout(0.1),\n    nn.Linear(hid_d, hid_d),\n).to(device)\n\n# 6-B  add GPT-2 blocks\nfor _ in range(NEW_GPT2_BLOCKS):\n    model.decoder.transformer.h.append(GPT2Block(model.decoder.config).to(device))\nnew_layers = len(model.decoder.transformer.h)\nfor obj in (model.decoder.transformer,\n            model.decoder.transformer.config,\n            model.decoder.config):\n    obj.n_layer = new_layers\n\n# 6-C  generation + smoothing\ncfg = model.config\ncfg.decoder_start_token_id = tokenizer.bos_token_id\ncfg.eos_token_id           = tokenizer.eos_token_id\ncfg.pad_token_id           = tokenizer.pad_token_id\ncfg.max_length             = MAX_SEQ_LEN\ncfg.no_repeat_ngram_size   = 5\ncfg.length_penalty         = 1.6\ncfg.early_stopping         = True\ncfg.label_smoothing        = LABEL_SMOOTH\ncfg.teacher_forcing_ratio  = 1.0   # start full TF\n\n# freeze encoder initially\nfor p in model.encoder.parameters(): p.requires_grad = False\nmodel.to(device)\n\n#####################################################################\n# 7.  Metrics – BLEU, ROUGE-L, CIDEr\n#####################################################################\nbleu   = load(\"bleu\")\nrouge  = load(\"rouge\")\nciderS = CiderScorer()\n\ndef postproc(preds, labels):\n    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels!=-100, labels, tokenizer.pad_token_id)\n    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return [p.strip() for p in preds], [[l.strip()] for l in labels]\n\ndef cider_score(preds, refs):\n    scorer = CiderScorer()\n    gts, res = {}, {}\n    for i,(p,r) in enumerate(zip(preds, refs)):\n        gts[i] = [x for x in r]\n        res[i] = [p]\n    sc,_ = scorer.compute_score(gts, res)\n    return sc\n\ndef compute_metrics(eval_pred):\n    p,l = postproc(*eval_pred)\n    out = bleu.compute(predictions=p, references=l)\n    out[\"rougeL\"] = rouge.compute(predictions=p,\n                     references=[r[0] for r in l],\n                     rouge_types=[\"rougeL\"])[\"rougeL\"]\n    out[\"cider\"]  = cider_score(p, l)\n    return out\n\n#####################################################################\n# 8.  Custom Trainer (2-group opt, gradual unfreeze, scheduled sampling)\n#####################################################################\nclass VCaptionTrainer(Seq2SeqTrainer):\n    def _setup_training(self,*a,**k):\n        super()._setup_training(*a,**k)\n        enc, dec = [], []\n        for n,p in self.model.named_parameters():\n            (enc if n.startswith(\"encoder.\") else dec).append(p)\n        self.optimizer = torch.optim.AdamW(\n            [{\"params\":enc,\"lr\":LR_ENCODER},\n             {\"params\":dec,\"lr\":LR_DECODER}],\n             lr=LR_DECODER, weight_decay=WEIGHT_DECAY)\n        warm = int(WARMUP_RATIO*self.args.max_steps)\n        self.lr_scheduler = get_cosine_schedule_with_warmup(\n            self.optimizer, warm, self.args.max_steps)\n\n    def get_train_dataloader(self):\n        # length-aware sampler\n        sampler = WeightedRandomSampler(\n            weights=self.train_dataset.dataset.weights[self.train_dataset.indices],\n            num_samples=len(self.train_dataset), replacement=True)\n        return torch.utils.data.DataLoader(\n            self.train_dataset, batch_size=self.args.per_device_train_batch_size,\n            sampler=sampler, collate_fn=default_data_collator)\n\n    def training_step(self, model, inputs):\n        # unfreeze ViT blocks gradually\n        if self.state.epoch is not None and self.state.epoch >= UNFREEZE_VIT_AFTER_EP:\n            for idx in range(12-UNFREEZE_VIT_LAST_N, 12):\n                for p in model.encoder.encoder.layer[idx].parameters():\n                    p.requires_grad = True\n            for g in self.optimizer.param_groups:\n                if g[\"lr\"] == LR_ENCODER:\n                    g[\"lr\"] = LR_ENCODER * 0.3      # e.g. 1.5 e-6\n        # linear teacher-forcing decay to 0.75\n        tf_ratio = max(0.75, 1.0 - 0.25*(self.state.epoch/(EPOCHS-1)))\n        model.config.teacher_forcing_ratio = tf_ratio\n        return super().training_step(model, inputs)\n\n#####################################################################\n# 9.  TrainingArguments\n#####################################################################\nargs = Seq2SeqTrainingArguments(\n    output_dir=\"vitgpt2_plus\",\n    per_device_train_batch_size=PER_DEVICE_BATCH,\n    per_device_eval_batch_size=PER_DEVICE_BATCH,\n    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n    num_train_epochs=EPOCHS,\n    predict_with_generate=True,\n    generation_max_length=MAX_SEQ_LEN,\n    generation_num_beams=4,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit = 1,\n    metric_for_best_model=\"cider\",\n    greater_is_better=True,\n    fp16=True,\n    fp16_full_eval=True,          # NEW – eval in fp16 avoids cast-mismatch\n    max_grad_norm=1.0,            # NEW – clip every backward() call\n    gradient_checkpointing=True,  # NEW – saves RAM *and* keeps grad norms small\n    logging_steps=50,\n    report_to=\"none\",\n)\n\ntrainer = VCaptionTrainer(\n    model=model, args=args,\n    train_dataset=train_ds, eval_dataset=val_ds,\n    tokenizer=processor,\n    data_collator=default_data_collator,\n    compute_metrics=compute_metrics)\n\n#####################################################################\n# 10.  Train & evaluate\n#####################################################################\ntrainer.train()\ntrainer.evaluate()\n\n#####################################################################\n# 11.  Quick inference helper\n#####################################################################\n# def caption_url(url, beams=4):\n#     model.eval()\n#     import requests, textwrap\n#     im = Image.open(BytesIO(requests.get(url, timeout=10).content)).convert(\"RGB\")\n#     px = processor(im, return_tensors=\"pt\").pixel_values.to(device)\n#     with torch.no_grad():\n#         out = model.generate(px, max_length=MAX_SEQ_LEN, num_beams=beams,\n#                              eos_token_id=tokenizer.eos_token_id,\n#                              pad_token_id=tokenizer.pad_token_id)\n#     return textwrap.shorten(tokenizer.decode(out[0], skip_special_tokens=True), 120)\n\n# Example:\n# print(caption_url(\"https://farm4.staticflickr.com/3629/3336631456_09f313e363.jpg\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:18:31.924306Z","iopub.execute_input":"2025-04-27T20:18:31.924991Z","execution_failed":"2025-04-28T03:28:24.538Z"}},"outputs":[{"name":"stdout","text":"Unique images: 8091   expanded rows: 40455\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"train 38432 | val 2023\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.bias', 'h.11.ln_cross_attn.bias', 'h.4.ln_cross_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.7.crossattention.c_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.10.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.crossattention.c_proj.bias', 'h.2.ln_cross_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.3.ln_cross_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.3.crossattention.c_attn.weight', 'h.10.ln_cross_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.0.ln_cross_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight', 'h.7.ln_cross_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.8.ln_cross_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.5.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.9.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.9.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.11.ln_cross_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.3.ln_cross_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.bias', 'h.0.ln_cross_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.10.ln_cross_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.5.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.6.ln_cross_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8607' max='9608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8607/9608 7:09:42 < 49:59, 0.33 it/s, Epoch 7.17/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Precisions</th>\n      <th>Brevity Penalty</th>\n      <th>Length Ratio</th>\n      <th>Translation Length</th>\n      <th>Reference Length</th>\n      <th>Rougel</th>\n      <th>Cider</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.374500</td>\n      <td>2.345213</td>\n      <td>0.046534</td>\n      <td>[0.2399133352709594, 0.0653751674854846, 0.026537676400106507, 0.011265655484926678]</td>\n      <td>1.000000</td>\n      <td>1.613669</td>\n      <td>37847</td>\n      <td>23454</td>\n      <td>0.269493</td>\n      <td>0.254084</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.174200</td>\n      <td>2.117774</td>\n      <td>0.054936</td>\n      <td>[0.25891952429203774, 0.07517122805039601, 0.03275944524151124, 0.014284350841472337]</td>\n      <td>1.000000</td>\n      <td>1.598960</td>\n      <td>37502</td>\n      <td>23454</td>\n      <td>0.279273</td>\n      <td>0.299178</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.029700</td>\n      <td>1.980820</td>\n      <td>0.060161</td>\n      <td>[0.2654248190833557, 0.08337348031853091, 0.03676647426647427, 0.016100636983451235]</td>\n      <td>1.000000</td>\n      <td>1.590773</td>\n      <td>37310</td>\n      <td>23454</td>\n      <td>0.281816</td>\n      <td>0.316163</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.926900</td>\n      <td>1.871343</td>\n      <td>0.063597</td>\n      <td>[0.2689347372922123, 0.08665065687243961, 0.038745055735347, 0.01811860027433092]</td>\n      <td>1.000000</td>\n      <td>1.595378</td>\n      <td>37418</td>\n      <td>23454</td>\n      <td>0.287508</td>\n      <td>0.325660</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.837700</td>\n      <td>1.792112</td>\n      <td>0.068179</td>\n      <td>[0.27371908841118475, 0.09091427106185361, 0.041933728383117666, 0.02070653399027469]</td>\n      <td>1.000000</td>\n      <td>1.582758</td>\n      <td>37122</td>\n      <td>23454</td>\n      <td>0.290879</td>\n      <td>0.347218</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.796700</td>\n      <td>1.723977</td>\n      <td>0.069103</td>\n      <td>[0.2740279686363269, 0.09307495012824167, 0.0433664983215895, 0.02061590001288494]</td>\n      <td>1.000000</td>\n      <td>1.582374</td>\n      <td>37113</td>\n      <td>23454</td>\n      <td>0.289821</td>\n      <td>0.339176</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.747900</td>\n      <td>1.681991</td>\n      <td>0.071028</td>\n      <td>[0.2750646830530401, 0.09426755223625324, 0.04440680016939924, 0.02210407604317706]</td>\n      <td>1.000000</td>\n      <td>1.581990</td>\n      <td>37104</td>\n      <td>23454</td>\n      <td>0.291821</td>\n      <td>0.352679</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null}]}