{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ResNet-50 and RNN","metadata":{}},{"cell_type":"code","source":"import os\nimport ssl\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nfrom collections import Counter\nimport requests\nfrom io import BytesIO\n\n# ---------------------------------------------------\n# Optional: disable SSL verification if SSL errors occur\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Download NLTK data (for tokenization)\nnltk.download('punkt')\n\n# ---------------------------------------------------\n# 1. Vocabulary and Tokenization\n# ---------------------------------------------------\nclass Vocabulary:\n    \"\"\"A simple vocabulary wrapper.\"\"\"\n    def __init__(self, freq_threshold):\n        self.freq_threshold = freq_threshold\n        # Keep punctuation tokens if they appear in the dataset\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n        self.stoi = {v: k for k, v in self.itos.items()}\n    \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer(text):\n        # We'll NOT remove punctuation here so that the model can learn punctuation tokens\n        text = text.lower().strip()\n        return nltk.tokenize.word_tokenize(text)  # keeps punctuation as separate tokens\n    \n    def build_vocabulary(self, sentence_list):\n        frequencies = Counter()\n        idx = 4  # starting index after special tokens\n        for sentence in sentence_list:\n            tokens = self.tokenizer(sentence)\n            frequencies.update(tokens)\n            for token in tokens:\n                # Only add a token to the vocab if its frequency == freq_threshold\n                if frequencies[token] == self.freq_threshold:\n                    # if it is not already in stoi\n                    if token not in self.stoi:\n                        self.stoi[token] = idx\n                        self.itos[idx] = token\n                        idx += 1\n    \n    def numericalize(self, text):\n        tokenized_text = self.tokenizer(text)\n        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized_text]\n\n# ---------------------------------------------------\n# 2. Hyperparameters and Transforms\n# ---------------------------------------------------\nembed_size    = 256\nhidden_size   = 512\nnum_layers    = 1\nlearning_rate = 1e-3\n# We'll train longer for a better model\nnum_epochs    = 10\n\nbatch_size    = 16\n# Raise the frequency threshold to reduce <unk>\nfreq_threshold = 5\n\nmax_seq_length = 30  # allowing a bit longer sentences\n\n# ImageNet-based transformations\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std =[0.229, 0.224, 0.225]\n    )\n])\n\n# ---------------------------------------------------\n# 3. Loading the CSV into a DataFrame\n# ---------------------------------------------------\ncsv_file = \"/kaggle/input/flickr8k/captions.txt\"  # your CSV with columns: image, caption\nimg_dir  = \"/kaggle/input/flickr8k/Images\"\n\ndf = pd.read_csv(csv_file)\nprint(\"Number of captions:\", len(df))\n\n# ---------------------------------------------------\n# 4. Dataset and DataLoader (with input/target split)\n# ---------------------------------------------------\nclass CaptionDatasetFromDF(Dataset):\n    def __init__(self, dataframe, img_dir, vocabulary, transform=None, max_seq_length=30):\n        self.df = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n        self.vocab = vocabulary\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.iloc[idx]['image']\n        caption  = self.df.iloc[idx]['caption']\n\n        # Load and transform image\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform is not None:\n            image = self.transform(image)\n\n        # Numericalize the caption\n        numericalized_caption = [self.vocab.stoi[\"<start>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<end>\"])\n\n        # Truncate if over max length\n        if len(numericalized_caption) > self.max_seq_length:\n            numericalized_caption = numericalized_caption[:self.max_seq_length]\n            numericalized_caption[-1] = self.vocab.stoi[\"<end>\"]\n\n        # Split into input & target\n        input_caption  = numericalized_caption[:-1]  # all but last\n        target_caption = numericalized_caption[1:]   # all but first\n\n        return image, torch.tensor(input_caption), torch.tensor(target_caption)\n\ndef collate_fn(data):\n    \"\"\"\n    Creates mini-batches by padding input and target captions to\n    the length of the longest caption in the batch.\n    \"\"\"\n    images, input_captions, target_captions = zip(*data)\n\n    # Stack images\n    images = torch.stack(images, 0)\n\n    # Pad input and target\n    padded_input  = pad_sequence(input_captions, batch_first=True, padding_value=0)\n    padded_target = pad_sequence(target_captions, batch_first=True, padding_value=0)\n\n    # The lengths are for reference only (not strictly needed with current code)\n    lengths = [len(cap) for cap in input_captions]\n\n    return images, padded_input, padded_target, lengths\n\n# ---------------------------------------------------\n# 5. Model Definitions\n# ---------------------------------------------------\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size, train_CNN=False):\n        super(EncoderCNN, self).__init__()\n        # Load a ResNet50\n        resnet = models.resnet50(pretrained=True)\n        modules = list(resnet.children())[:-1]  # Remove final FC\n        self.resnet = nn.Sequential(*modules)\n\n        # Optionally freeze the CNN\n        for param in self.resnet.parameters():\n            param.requires_grad = train_CNN\n\n        # Map 2048 -> embed_size\n        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n        self.bn     = nn.BatchNorm1d(embed_size, momentum=0.01)\n        \n    def forward(self, images):\n        features = self.resnet(images)                # (batch, 2048, 1, 1)\n        features = features.view(features.size(0), -1)  # (batch, 2048)\n        features = self.linear(features)                 # (batch, embed_size)\n        features = self.bn(features)\n        return features\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, max_seq_length=30):\n        super(DecoderRNN, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm  = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.max_seq_length = max_seq_length\n\n        self.vocab_size = vocab_size\n\n    def forward(self, features, captions):\n        # features: (batch, embed_size)\n        # captions: (batch, seq_len)\n        embeddings = self.embed(captions)  # (batch, seq_len, embed_size)\n        # Insert the image features at t=0\n        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)  # (batch, seq_len+1, embed_size)\n\n        outputs, _ = self.lstm(embeddings)      # (batch, seq_len+1, hidden_size)\n        outputs = self.linear(outputs)          # (batch, seq_len+1, vocab_size)\n        outputs = outputs[:, 1:, :]            # drop the first time step => (batch, seq_len, vocab_size)\n        return outputs\n\n    def sample_greedy(self, features):\n        \"\"\"(Optional) Greedy search as an alternative to beam search.\"\"\"\n        sampled_ids = []\n        inputs = features.unsqueeze(1)  # (batch, 1, embed_size)\n\n        for _ in range(self.max_seq_length):\n            hiddens, states = self.lstm(inputs)\n            outputs = self.linear(hiddens.squeeze(1))  # (batch, vocab_size)\n            predicted = outputs.argmax(dim=1)          # (batch)\n            sampled_ids.append(predicted)\n            inputs = self.embed(predicted).unsqueeze(1)\n\n        sampled_ids = torch.stack(sampled_ids, 1)\n        return sampled_ids\n\n    def sample_beam_search(self, features, vocab, beam_size=3):\n        \"\"\"\n        Beam search for improved decoding.\n        Returns the best predicted sequence of token IDs (excluding <start>).\n        \"\"\"\n        device = features.device\n        start_id = vocab.stoi[\"<start>\"]\n        end_id   = vocab.stoi[\"<end>\"]\n\n        # Each element: (sequence_of_ids, log_prob, hidden_state, cell_state)\n        sequences = [([start_id], 0.0, None, None)]\n\n        for _ in range(self.max_seq_length):\n            all_candidates = []\n            for seq, log_prob, hidden, cell in sequences:\n                # If the last token was <end>, just keep adding it as is\n                if seq[-1] == end_id:\n                    all_candidates.append((seq, log_prob, hidden, cell))\n                    continue\n\n                # The last token\n                last_token = torch.tensor([seq[-1]]).to(device)\n                # embed that token\n                token_embed = self.embed(last_token).unsqueeze(1)  # shape: (1,1,embed_size)\n\n                if hidden is None and cell is None:\n                    # We'll treat the image features as the initial hidden\n                    # so we pass the image feature + token as the first input\n                    # We can do a single-step LSTM pass\n                    init_input = torch.cat((features.unsqueeze(1), token_embed), dim=1)  # shape (1,2,embed_size)\n                    lstm_out, (hidden_next, cell_next) = self.lstm(init_input)\n                    # last step output\n                    lstm_out = lstm_out[:, -1, :]\n                else:\n                    lstm_out, (hidden_next, cell_next) = self.lstm(token_embed, (hidden, cell))\n\n                scores = self.linear(lstm_out.squeeze(1))   # shape (1, vocab_size)\n                log_probs = torch.log_softmax(scores, dim=1)\n\n                # get top beam_size expansions\n                topk_log_probs, topk_ids = torch.topk(log_probs, beam_size, dim=1)\n                for i in range(beam_size):\n                    candidate_seq = seq + [topk_ids[0, i].item()]\n                    candidate_log_prob = log_prob + topk_log_probs[0, i].item()\n                    all_candidates.append((candidate_seq, candidate_log_prob, hidden_next, cell_next))\n\n            # sort all candidates in descending order of log_prob\n            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n            # take top beam_size\n            sequences = ordered[:beam_size]\n\n        # choose best sequence\n        best_seq, best_logprob, _, _ = sequences[0]\n        # remove the <start> token\n        return best_seq[1:]  # these are token IDs\n\n# Full model wrapper\nclass ImageCaptioningModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(ImageCaptioningModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs\n\n# ---------------------------------------------------\n# 6. Training function with .reshape\n# ---------------------------------------------------\ndef train(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n\n    for images, input_captions, target_captions, _ in dataloader:\n        images          = images.to(device)\n        input_captions  = input_captions.to(device)\n        target_captions = target_captions.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        features = model.encoder(images)\n        outputs  = model.decoder(features, input_captions)\n        # outputs:  (batch, seq_len, vocab_size)\n        # targets:  (batch, seq_len)\n\n        # Flatten\n        outputs_reshaped = outputs.reshape(-1, outputs.size(2))\n        targets_reshaped = target_captions.reshape(-1)\n\n        loss = criterion(outputs_reshaped, targets_reshaped)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\n# ---------------------------------------------------\n# 7. Multi-Sentence Postprocessing\n# ---------------------------------------------------\ndef tokens_to_multisentence(token_ids, vocab, max_sentences=3):\n    \"\"\"\n    Convert a list of token IDs to up to 'max_sentences' sentences, \n    splitting on '.' or <end> token. Returns a string with multiple sentences.\n    \"\"\"\n    sentences = []\n    current_sentence = []\n\n    end_token_id  = vocab.stoi[\"<end>\"]\n\n    for token_id in token_ids:\n        if token_id == end_token_id:\n            # end token => finish up\n            if current_sentence:\n                sentences.append(\" \".join(current_sentence))\n            break\n\n        word = vocab.itos.get(token_id, \"<unk>\")\n        if word == \"<unk>\":\n            # Optionally skip or replace <unk>\n            word = \"something\"\n\n        current_sentence.append(word)\n\n        # If we see a '.' we treat it as a sentence boundary\n        if word == \".\":\n            sentences.append(\" \".join(current_sentence))\n            current_sentence = []\n\n        # If we already have max_sentences\n        if len(sentences) >= max_sentences:\n            break\n\n    # If there's leftover text that didn't end with '.', we can finalize it\n    if current_sentence and len(sentences) < max_sentences:\n        sentences.append(\" \".join(current_sentence))\n\n    # Join with a period + space or newlines, whichever you prefer\n    final_output = \". \".join(sentences)\n    # Optionally ensure punctuation at the end\n    if not final_output.endswith(\".\"):\n        final_output += \".\"\n\n    return final_output\n\n# ---------------------------------------------------\n# 8. Prediction with Beam Search\n# ---------------------------------------------------\ndef predict_caption_from_url(model, url, transform, vocab, device, beam_size=3, max_sentences=3):\n    model.eval()\n    response = requests.get(url)\n    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    image = transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        features = model.encoder(image)\n        # We'll do beam search\n        token_ids = model.decoder.sample_beam_search(features, vocab, beam_size=beam_size)\n\n    # Convert token IDs to multi-sentence text\n    caption_text = tokens_to_multisentence(token_ids, vocab, max_sentences=max_sentences)\n    return caption_text\n\n# ---------------------------------------------------\n# 9. Execution: Build, Train, Predict\n# ---------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 9.1: Build the vocabulary\ncaptions_list = df['caption'].tolist()\nvocab = Vocabulary(freq_threshold)\nvocab.build_vocabulary(captions_list)\nprint(\"Vocabulary size:\", len(vocab))\n\n# 9.2: Dataset & DataLoader\ndataset = CaptionDatasetFromDF(df, img_dir, vocab, transform=transform, max_seq_length=max_seq_length)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\n# 9.3: Model Instantiation\nencoder = EncoderCNN(embed_size).to(device)\ndecoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers, max_seq_length).to(device)\nmodel   = ImageCaptioningModel(encoder, decoder).to(device)\n\n# 9.4: Loss + Optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# 9.5: Training\nprint(\"Starting training ...\")\nfor epoch in range(num_epochs):\n    epoch_loss = train(model, dataloader, criterion, optimizer, device)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n\n# 9.6: Save the model\ntorch.save(model.state_dict(), \"image_captioning_model.pth\")\nprint(\"Model saved!\")\n\n# 9.7: Prediction from URL\ntest_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(\n    model, \n    test_url, \n    transform, \n    vocab, \n    device,\n    beam_size=3,      # you can experiment with beam sizes\n    max_sentences=3   # we want up to 3 sentences\n)\nprint(\"Predicted Multi-Sentence Caption:\", predicted_caption)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T17:51:15.359656Z","iopub.execute_input":"2025-04-02T17:51:15.359946Z","iopub.status.idle":"2025-04-02T18:55:04.649740Z","shell.execute_reply.started":"2025-04-02T17:51:15.359926Z","shell.execute_reply":"2025-04-02T18:55:04.649028Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nNumber of captions: 40455\nVocabulary size: 2984\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 240MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Starting training ...\nEpoch [1/10], Loss: 3.0775\nEpoch [2/10], Loss: 2.4914\nEpoch [3/10], Loss: 2.2401\nEpoch [4/10], Loss: 2.0401\nEpoch [5/10], Loss: 1.8669\nEpoch [6/10], Loss: 1.7148\nEpoch [7/10], Loss: 1.5836\nEpoch [8/10], Loss: 1.4687\nEpoch [9/10], Loss: 1.3687\nEpoch [10/10], Loss: 1.2843\nModel saved!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an image URL for caption prediction:  https://cdn.prod.website-files.com/5aba1faad4eb88cb5f8c0b57/5acd3f675e259d1e064464ae_imgonline-com-ua-CompressToSize-OOr5aNN1GIUCTB2.jpg\n"},{"name":"stdout","text":"Predicted Multi-Sentence Caption: three people are standing on a grassy hillside .\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"test_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(\n    model, \n    test_url, \n    transform, \n    vocab, \n    device,\n    beam_size=3,      # you can experiment with beam sizes\n    max_sentences=3   # we want up to 3 sentences\n)\nprint(\"Predicted Multi-Sentence Caption:\", predicted_caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:56:15.976489Z","iopub.execute_input":"2025-04-02T18:56:15.976776Z","iopub.status.idle":"2025-04-02T18:56:17.580087Z","shell.execute_reply.started":"2025-04-02T18:56:15.976756Z","shell.execute_reply":"2025-04-02T18:56:17.579460Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter an image URL for caption prediction:  https://hips.hearstapps.com/hmg-prod/images/gettyimages-180680638-676f621f720bc.jpg?crop=0.8888888888888888xw:1xh;center,top&resize=1200:*\n"},{"name":"stdout","text":"Predicted Multi-Sentence Caption: three dogs run through the grass .\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# https://hips.hearstapps.com/hmg-prod/images/gettyimages-180680638-676f621f720bc.jpg?crop=0.8888888888888888xw:1xh;center,top&resize=1200:*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T18:56:34.820066Z","iopub.execute_input":"2025-04-02T18:56:34.820435Z","iopub.status.idle":"2025-04-02T18:56:34.823968Z","shell.execute_reply.started":"2025-04-02T18:56:34.820371Z","shell.execute_reply":"2025-04-02T18:56:34.823234Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# MobileNet-V3 and GRU","metadata":{}},{"cell_type":"code","source":"import os\nimport ssl\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nfrom collections import Counter\nimport requests\nfrom io import BytesIO\n\n# ---------------------------------------------------\n# Optional: disable SSL verification if SSL errors occur\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Download NLTK data (for tokenization)\nnltk.download('punkt')\n\n# ---------------------------------------------------\n# 1. Vocabulary and Tokenization\n# ---------------------------------------------------\nclass Vocabulary:\n    \"\"\"A simple vocabulary wrapper.\"\"\"\n    def __init__(self, freq_threshold):\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n        self.stoi = {v: k for k, v in self.itos.items()}\n    \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer(text):\n        # Keep punctuation so the model can learn multi-sentence outputs\n        text = text.lower().strip()\n        return nltk.tokenize.word_tokenize(text)\n    \n    def build_vocabulary(self, sentence_list):\n        frequencies = Counter()\n        idx = 4  # starting index after special tokens\n        for sentence in sentence_list:\n            tokens = self.tokenizer(sentence)\n            frequencies.update(tokens)\n            for token in tokens:\n                if frequencies[token] == self.freq_threshold:\n                    if token not in self.stoi:\n                        self.stoi[token] = idx\n                        self.itos[idx] = token\n                        idx += 1\n    \n    def numericalize(self, text):\n        tokenized_text = self.tokenizer(text)\n        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized_text]\n\n# ---------------------------------------------------\n# 2. Hyperparameters and Transforms\n# ---------------------------------------------------\nembed_size     = 256\nhidden_size    = 512\nnum_layers     = 1\nlearning_rate  = 1e-3\nnum_epochs     = 10  # train longer for better results\nbatch_size     = 16\nfreq_threshold = 5   # raise threshold to reduce <unk>\nmax_seq_length = 30  # allow slightly longer sequences\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std =[0.229, 0.224, 0.225]\n    )\n])\n\n# ---------------------------------------------------\n# 3. Load CSV with image, caption columns\n# ---------------------------------------------------\ncsv_file = \"/kaggle/input/flickr8k/captions.txt\"\nimg_dir  = \"/kaggle/input/flickr8k/Images\"\n\ndf = pd.read_csv(csv_file)\nprint(\"Number of captions:\", len(df))\n\n# ---------------------------------------------------\n# 4. Dataset and DataLoader\n# ---------------------------------------------------\nclass CaptionDatasetFromDF(Dataset):\n    def __init__(self, dataframe, img_dir, vocabulary, transform=None, max_seq_length=30):\n        self.df = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n        self.vocab = vocabulary\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.iloc[idx]['image']\n        caption  = self.df.iloc[idx]['caption']\n\n        # Load and transform the image\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform is not None:\n            image = self.transform(image)\n\n        # Numericalize the caption\n        numericalized_caption = [self.vocab.stoi[\"<start>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<end>\"])\n\n        # Truncate if needed\n        if len(numericalized_caption) > self.max_seq_length:\n            numericalized_caption = numericalized_caption[:self.max_seq_length]\n            numericalized_caption[-1] = self.vocab.stoi[\"<end>\"]\n\n        # Create input & target\n        input_caption  = numericalized_caption[:-1]\n        target_caption = numericalized_caption[1:]\n\n        return image, torch.tensor(input_caption), torch.tensor(target_caption)\n\ndef collate_fn(data):\n    images, input_captions, target_captions = zip(*data)\n\n    images = torch.stack(images, 0)\n    padded_input  = pad_sequence(input_captions, batch_first=True, padding_value=0)\n    padded_target = pad_sequence(target_captions, batch_first=True, padding_value=0)\n\n    lengths = [len(cap) for cap in input_captions]  # optional\n\n    return images, padded_input, padded_target, lengths\n\n# ---------------------------------------------------\n# 5. Model Definitions\n# ---------------------------------------------------\n#\n# -- New CNN: MobileNetV3 Large --\n#\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size, train_CNN=False):\n        super(EncoderCNN, self).__init__()\n        # Use MobileNet V3 Large as a feature extractor\n        backbone = models.mobilenet_v3_large(pretrained=True)\n        \n        # Remove the final classifier; keep only the feature layers\n        self.backbone = backbone.features  # (batch, 960, H', W')\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # convert H'xW' -> 1x1\n\n        # Optionally freeze layers\n        for param in self.backbone.parameters():\n            param.requires_grad = train_CNN\n\n        # Linear to map from 960 -> embed_size\n        self.linear = nn.Linear(960, embed_size)\n        self.bn     = nn.BatchNorm1d(embed_size, momentum=0.01)\n\n    def forward(self, images):\n        x = self.backbone(images)           # shape: (batch, 960, H', W')\n        x = self.global_pool(x)             # (batch, 960, 1, 1)\n        x = x.view(x.size(0), -1)           # (batch, 960)\n        x = self.linear(x)                  # (batch, embed_size)\n        x = self.bn(x)\n        return x\n\n#\n# -- New RNN: GRU for text decoding --\n#\nclass DecoderGRU(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, max_seq_length=30):\n        super(DecoderGRU, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.gru   = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.max_seq_length = max_seq_length\n\n        self.vocab_size = vocab_size\n\n    def forward(self, features, captions):\n        \"\"\"\n        features: (batch, embed_size)\n        captions: (batch, seq_len)\n        \"\"\"\n        embeddings = self.embed(captions)  # (batch, seq_len, embed_size)\n        # Insert image features at t=0\n        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)  \n        # (batch, seq_len+1, embed_size)\n\n        outputs, _ = self.gru(embeddings)    # (batch, seq_len+1, hidden_size)\n        outputs = self.linear(outputs)       # (batch, seq_len+1, vocab_size)\n        outputs = outputs[:, 1:, :]         # drop the first time step\n        return outputs\n\n    # Optional: A simpler greedy approach (if you want)\n    def sample_greedy(self, features):\n        sampled_ids = []\n        inputs = features.unsqueeze(1)  # (batch, 1, embed_size)\n        h = None\n\n        for _ in range(self.max_seq_length):\n            out, h = self.gru(inputs, h)      # (batch, 1, hidden_size)\n            out = self.linear(out.squeeze(1)) # (batch, vocab_size)\n            predicted = out.argmax(dim=1)     # (batch)\n            sampled_ids.append(predicted)\n            inputs = self.embed(predicted).unsqueeze(1)\n\n        sampled_ids = torch.stack(sampled_ids, 1)\n        return sampled_ids\n\n    def sample_beam_search(self, features, vocab, beam_size=3):\n        \"\"\"\n        Beam Search for better decoding.\n        Returns the best predicted sequence of token IDs (excluding <start>).\n        \"\"\"\n        device = features.device\n        start_id = vocab.stoi[\"<start>\"]\n        end_id   = vocab.stoi[\"<end>\"]\n\n        # Each item is (seq, log_prob, hidden)\n        sequences = [([start_id], 0.0, None)]\n\n        for _ in range(self.max_seq_length):\n            all_candidates = []\n            for seq, log_prob, hidden in sequences:\n                if seq[-1] == end_id:\n                    # If already ended, keep as-is\n                    all_candidates.append((seq, log_prob, hidden))\n                    continue\n\n                last_token = torch.tensor([seq[-1]]).to(device)\n                token_embed = self.embed(last_token).unsqueeze(1)  # (1,1,embed_size)\n\n                if hidden is None:\n                    # treat image features as initial hidden; let's do 1 step\n                    # We can do a dummy forward pass with: \n                    #   out, new_hidden = self.gru(torch.cat((features.unsqueeze(1), token_embed), 1))\n                    # but that implies 2 time steps at once. Alternatively, we do:\n                    out, new_hidden = self.gru(token_embed, None)\n                    # Combine with features as if it's the first token if desired.\n                    # But simpler is to just pass token_embed alone each step.\n                else:\n                    out, new_hidden = self.gru(token_embed, hidden)\n\n                out = self.linear(out.squeeze(1))  # (1, vocab_size)\n                log_probs = torch.log_softmax(out, dim=1)\n\n                topk_log_probs, topk_ids = torch.topk(log_probs, beam_size, dim=1)\n                for i in range(beam_size):\n                    candidate_seq = seq + [topk_ids[0, i].item()]\n                    candidate_log_prob = log_prob + topk_log_probs[0, i].item()\n                    all_candidates.append((candidate_seq, candidate_log_prob, new_hidden))\n\n            # Re-sort\n            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n            sequences = ordered[:beam_size]\n\n        best_seq, best_logprob, _ = sequences[0]\n        return best_seq[1:]  # remove the <start> token\n\nclass ImageCaptioningModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(ImageCaptioningModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs\n\n# ---------------------------------------------------\n# 6. Training function\n# ---------------------------------------------------\ndef train(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    for images, input_captions, target_captions, _ in dataloader:\n        images = images.to(device)\n        input_captions = input_captions.to(device)\n        target_captions = target_captions.to(device)\n\n        optimizer.zero_grad()\n\n        features = model.encoder(images)\n        outputs  = model.decoder(features, input_captions)\n        # (batch, seq_len, vocab_size) vs (batch, seq_len)\n\n        # Flatten\n        outputs_reshaped = outputs.reshape(-1, outputs.size(2))\n        targets_reshaped = target_captions.reshape(-1)\n\n        loss = criterion(outputs_reshaped, targets_reshaped)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\n# ---------------------------------------------------\n# 7. Multi-Sentence Postprocessing\n# ---------------------------------------------------\ndef tokens_to_multisentence(token_ids, vocab, max_sentences=3):\n    \"\"\"\n    Converts token IDs to multiple sentences, splitting on '.' or <end>.\n    \"\"\"\n    sentences = []\n    current_sentence = []\n    end_token_id = vocab.stoi[\"<end>\"]\n\n    for token_id in token_ids:\n        if token_id == end_token_id:\n            if current_sentence:\n                sentences.append(\" \".join(current_sentence))\n            break\n\n        word = vocab.itos.get(token_id, \"<unk>\")\n        if word == \"<unk>\":\n            # skip or replace <unk>\n            word = \"something\"\n\n        current_sentence.append(word)\n\n        if word == \".\":\n            sentences.append(\" \".join(current_sentence))\n            current_sentence = []\n\n        if len(sentences) >= max_sentences:\n            break\n\n    if current_sentence and len(sentences) < max_sentences:\n        sentences.append(\" \".join(current_sentence))\n\n    final_output = \". \".join(sentences)\n    if not final_output.endswith(\".\"):\n        final_output += \".\"\n\n    return final_output\n\n# ---------------------------------------------------\n# 8. Prediction with Beam Search\n# ---------------------------------------------------\ndef predict_caption_from_url(model, url, transform, vocab, device, beam_size=3, max_sentences=3):\n    model.eval()\n    response = requests.get(url)\n    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    image = transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        features = model.encoder(image)\n        token_ids = model.decoder.sample_beam_search(features, vocab, beam_size=beam_size)\n\n    # Convert token IDs to a multi-sentence string\n    caption_text = tokens_to_multisentence(token_ids, vocab, max_sentences=max_sentences)\n    return caption_text\n\n# ---------------------------------------------------\n# 9. Execution: Build, Train, Predict\n# ---------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 9.1: Build the vocabulary\ncaptions_list = df['caption'].tolist()\nvocab = Vocabulary(freq_threshold)\nvocab.build_vocabulary(captions_list)\nprint(\"Vocabulary size:\", len(vocab))\n\n# 9.2: Dataset & DataLoader\ndataset = CaptionDatasetFromDF(df, img_dir, vocab, transform=transform, max_seq_length=max_seq_length)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\n# 9.3: Instantiate the MobileNetV3 + GRU model\nencoder = EncoderCNN(embed_size).to(device)\ndecoder = DecoderGRU(embed_size, hidden_size, len(vocab), num_layers, max_seq_length).to(device)\nmodel   = ImageCaptioningModel(encoder, decoder).to(device)\n\n# 9.4: Loss + Optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# 9.5: Training\nprint(\"Starting training ...\")\nfor epoch in range(num_epochs):\n    epoch_loss = train(model, dataloader, criterion, optimizer, device)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n\n# 9.6: Save the model\ntorch.save(model.state_dict(), \"image_captioning_mobilenet_gru.pth\")\nprint(\"Model saved!\")\n\n# 9.7: Prediction from URL\ntest_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(\n    model, \n    test_url, \n    transform, \n    vocab, \n    device,\n    beam_size=3,      # can be tuned\n    max_sentences=3   # request up to 3 sentences\n)\nprint(\"Predicted Multi-Sentence Caption:\", predicted_caption)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T23:47:56.145885Z","iopub.execute_input":"2025-04-03T23:47:56.146169Z","iopub.status.idle":"2025-04-04T00:52:28.184552Z","shell.execute_reply.started":"2025-04-03T23:47:56.146136Z","shell.execute_reply":"2025-04-04T00:52:28.183553Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nNumber of captions: 40455\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n","output_type":"stream"},{"name":"stdout","text":"Vocabulary size: 2984\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 21.1M/21.1M [00:00<00:00, 169MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Starting training ...\nEpoch [1/10], Loss: 2.9724\nEpoch [2/10], Loss: 2.4165\nEpoch [3/10], Loss: 2.1717\nEpoch [4/10], Loss: 1.9876\nEpoch [5/10], Loss: 1.8380\nEpoch [6/10], Loss: 1.7148\nEpoch [7/10], Loss: 1.6178\nEpoch [8/10], Loss: 1.5368\nEpoch [9/10], Loss: 1.4673\nEpoch [10/10], Loss: 1.4130\nModel saved!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter an image URL for caption prediction:  https://hips.hearstapps.com/hmg-prod/images/gettyimages-180680638-676f621f720bc.jpg?crop=0.8888888888888888xw:1xh;center,top&resize=1200:*\n"},{"name":"stdout","text":"Predicted Multi-Sentence Caption: a man and a woman are sitting on a bench outside .\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"test_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(\n    model, \n    test_url, \n    transform, \n    vocab, \n    device,\n    beam_size=3,      # can be tuned\n    max_sentences=3   # request up to 3 sentences\n)\nprint(\"Predicted Multi-Sentence Caption:\", predicted_caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T00:53:41.230748Z","iopub.execute_input":"2025-04-04T00:53:41.231082Z","iopub.status.idle":"2025-04-04T00:53:43.225531Z","shell.execute_reply.started":"2025-04-04T00:53:41.231057Z","shell.execute_reply":"2025-04-04T00:53:43.224673Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter an image URL for caption prediction:  https://images.pexels.com/photos/1054655/pexels-photo-1054655.jpeg?cs=srgb&dl=pexels-hsapir-1054655.jpg&fm=jpg\n"},{"name":"stdout","text":"Predicted Multi-Sentence Caption: a man and a woman are sitting on a bench outside .\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"test_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(\n    model, \n    test_url, \n    transform, \n    vocab, \n    device,\n    beam_size=3,      # can be tuned\n    max_sentences=3   # request up to 3 sentences\n)\nprint(\"Predicted Multi-Sentence Caption:\", predicted_caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T00:55:52.066489Z","iopub.execute_input":"2025-04-04T00:55:52.066882Z","iopub.status.idle":"2025-04-04T00:55:53.550641Z","shell.execute_reply.started":"2025-04-04T00:55:52.066855Z","shell.execute_reply":"2025-04-04T00:55:53.549789Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter an image URL for caption prediction:  https://plus.unsplash.com/premium_photo-1664474619075-644dd191935f?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8MXx8aW1hZ2V8ZW58MHx8MHx8fDA%3D\n"},{"name":"stdout","text":"Predicted Multi-Sentence Caption: a man and a woman are sitting on a bench outside .\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Better version","metadata":{}},{"cell_type":"code","source":"import os\nimport ssl\nimport nltk\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nfrom collections import Counter\nimport requests\nfrom io import BytesIO\n\n# -------------------------------------------------------------------------\n# 0. (Optional) Fix random seeds for reproducibility\n# -------------------------------------------------------------------------\ndef set_random_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # For cudnn reproducibility\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(42)\n\n# -------------------------------------------------------------------------\n# Optional: disable SSL verification if SSL errors occur\n# -------------------------------------------------------------------------\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Download NLTK data (for tokenization)\nnltk.download('punkt')\n\n# -------------------------------------------------------------------------\n# 1. Vocabulary and Tokenization\n# -------------------------------------------------------------------------\nclass Vocabulary:\n    \"\"\"A simple vocabulary wrapper.\"\"\"\n    def __init__(self, freq_threshold):\n        self.freq_threshold = freq_threshold\n        # Keep punctuation tokens\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n        self.stoi = {v: k for k, v in self.itos.items()}\n    \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer(text):\n        text = text.lower().strip()\n        return nltk.tokenize.word_tokenize(text)\n    \n    def build_vocabulary(self, sentence_list):\n        frequencies = Counter()\n        idx = 4  # starting index after special tokens\n        for sentence in sentence_list:\n            tokens = self.tokenizer(sentence)\n            frequencies.update(tokens)\n            for token in tokens:\n                if frequencies[token] == self.freq_threshold:\n                    if token not in self.stoi:\n                        self.stoi[token] = idx\n                        self.itos[idx] = token\n                        idx += 1\n    \n    def numericalize(self, text):\n        tokenized_text = self.tokenizer(text)\n        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized_text]\n\n# -------------------------------------------------------------------------\n# 2. Hyperparameters and Data Augmentations\n# -------------------------------------------------------------------------\nembed_size     = 256\nhidden_size    = 512\nnum_layers     = 1\nlearning_rate  = 1e-3\nnum_epochs     = 15  # increased for better training\nbatch_size     = 16\nfreq_threshold = 5\nmax_seq_length = 30\n\n# Data Augmentation + Normalization\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomCrop((224,224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\n# For inference, no augmentations\ninference_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\n# -------------------------------------------------------------------------\n# 3. Load CSV with image, caption columns\n# -------------------------------------------------------------------------\ncsv_file = \"/kaggle/input/flickr8k/captions.txt\"\nimg_dir  = \"/kaggle/input/flickr8k/Images\"\n\ndf = pd.read_csv(csv_file)\nprint(\"Number of captions:\", len(df))\n\n# -------------------------------------------------------------------------\n# 4. Dataset and DataLoader\n# -------------------------------------------------------------------------\nclass CaptionDatasetFromDF(Dataset):\n    def __init__(self, dataframe, img_dir, vocabulary, transform=None, max_seq_length=30):\n        self.df = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n        self.vocab = vocabulary\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.iloc[idx]['image']\n        caption  = self.df.iloc[idx]['caption']\n\n        # Load image\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        # Numericalize\n        numericalized_caption = [self.vocab.stoi[\"<start>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<end>\"])\n\n        # Truncate if needed\n        if len(numericalized_caption) > self.max_seq_length:\n            numericalized_caption = numericalized_caption[:self.max_seq_length]\n            numericalized_caption[-1] = self.vocab.stoi[\"<end>\"]\n\n        input_caption  = numericalized_caption[:-1]\n        target_caption = numericalized_caption[1:]\n\n        return image, torch.tensor(input_caption), torch.tensor(target_caption)\n\ndef collate_fn(data):\n    images, input_captions, target_captions = zip(*data)\n\n    images = torch.stack(images, 0)\n    padded_input  = pad_sequence(input_captions, batch_first=True, padding_value=0)\n    padded_target = pad_sequence(target_captions, batch_first=True, padding_value=0)\n\n    lengths = [len(cap) for cap in input_captions]\n    return images, padded_input, padded_target, lengths\n\n# -------------------------------------------------------------------------\n# 5. Model Definitions (MobileNetV3 + GRU with partial fine-tuning)\n# -------------------------------------------------------------------------\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        backbone = models.mobilenet_v3_large(pretrained=True)\n        # Keep the feature extraction layers\n        self.backbone = backbone.features  \n        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n        # Unfreeze only the last few layers for partial fine-tuning:\n        for name, param in self.backbone.named_parameters():\n            # Example: unfreeze last stage\n            if \"12\" in name or \"13\" in name or \"14\" in name:  # mobilenet_v3_large has final layers around 14\n                param.requires_grad = True\n            else:\n                param.requires_grad = False\n\n        # Map from 960 -> embed_size\n        self.linear = nn.Linear(960, embed_size)\n        self.bn     = nn.BatchNorm1d(embed_size, momentum=0.01)\n\n    def forward(self, images):\n        x = self.backbone(images)     # (batch, 960, H, W)\n        x = self.global_pool(x)       # (batch, 960, 1, 1)\n        x = x.view(x.size(0), -1)     # (batch, 960)\n        x = self.linear(x)            # (batch, embed_size)\n        x = self.bn(x)\n        return x\n\nclass DecoderGRU(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, dropout=0.3, num_layers=1, max_seq_length=30):\n        super(DecoderGRU, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        # Add dropout to reduce overfitting\n        self.gru   = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.max_seq_length = max_seq_length\n        self.vocab_size = vocab_size\n\n    def forward(self, features, captions):\n        embeddings = self.embed(captions)\n        # Insert the image features at t=0\n        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n        outputs, _ = self.gru(embeddings)\n        outputs = self.linear(outputs)\n        outputs = outputs[:, 1:, :]  # drop first time step\n        return outputs\n\n    def sample_beam_search(self, features, vocab, beam_size=3):\n        \"\"\"\n        Beam Search decoding.\n        \"\"\"\n        device = features.device\n        start_id = vocab.stoi[\"<start>\"]\n        end_id   = vocab.stoi[\"<end>\"]\n        sequences = [([start_id], 0.0, None)]\n\n        for _ in range(self.max_seq_length):\n            all_candidates = []\n            for seq, log_prob, hidden in sequences:\n                if seq[-1] == end_id:\n                    all_candidates.append((seq, log_prob, hidden))\n                    continue\n\n                last_token = torch.tensor([seq[-1]]).to(device)\n                token_embed = self.embed(last_token).unsqueeze(1)\n                \n                if hidden is None:\n                    out, new_hidden = self.gru(token_embed, None)\n                else:\n                    out, new_hidden = self.gru(token_embed, hidden)\n\n                out = self.linear(out.squeeze(1))  \n                log_probs = torch.log_softmax(out, dim=1)\n\n                topk_log_probs, topk_ids = torch.topk(log_probs, beam_size, dim=1)\n                for i in range(beam_size):\n                    candidate_seq = seq + [topk_ids[0, i].item()]\n                    candidate_log_prob = log_prob + topk_log_probs[0, i].item()\n                    all_candidates.append((candidate_seq, candidate_log_prob, new_hidden))\n\n            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n            sequences = ordered[:beam_size]\n\n        best_seq, best_logprob, _ = sequences[0]\n        return best_seq[1:]  # remove <start>\n\n    # Optional: top-k sampling for variety\n    def sample_topk(self, features, vocab, k=5, temperature=1.0):\n        device = features.device\n        start_id = vocab.stoi[\"<start>\"]\n        end_id   = vocab.stoi[\"<end>\"]\n        \n        sampled_ids = []\n        hidden = None\n\n        token_id = torch.tensor([start_id], device=device)\n        inputs   = self.embed(token_id).unsqueeze(1)\n        \n        for _ in range(self.max_seq_length):\n            if hidden is None:\n                out, hidden = self.gru(inputs)\n            else:\n                out, hidden = self.gru(inputs, hidden)\n\n            out = self.linear(out.squeeze(1))  # (1, vocab_size)\n            # Scale by temperature\n            out = out / temperature\n            probs = torch.softmax(out, dim=1)\n            # top-k sampling\n            topk_probs, topk_ids = torch.topk(probs, k, dim=1)\n            topk_probs = topk_probs.squeeze(0)\n            topk_ids   = topk_ids.squeeze(0)\n\n            # pick 1 from topk by random\n            chosen_idx = torch.multinomial(topk_probs, 1)\n            chosen_id  = topk_ids[chosen_idx]\n\n            sampled_ids.append(chosen_id.item())\n            if chosen_id.item() == end_id:\n                break\n\n            # next input\n            inputs = self.embed(chosen_id).unsqueeze(1)\n\n        return sampled_ids\n\nclass ImageCaptioningModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(ImageCaptioningModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs\n\n# -------------------------------------------------------------------------\n# 6. Training loop\n# -------------------------------------------------------------------------\ndef train(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    for images, input_captions, target_captions, _ in dataloader:\n        images          = images.to(device)\n        input_captions  = input_captions.to(device)\n        target_captions = target_captions.to(device)\n\n        optimizer.zero_grad()\n        features = model.encoder(images)\n        outputs  = model.decoder(features, input_captions)\n        # (batch, seq_len, vocab_size) vs. (batch, seq_len)\n\n        outputs_reshaped = outputs.reshape(-1, outputs.size(2))\n        targets_reshaped = target_captions.reshape(-1)\n\n        loss = criterion(outputs_reshaped, targets_reshaped)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\n# -------------------------------------------------------------------------\n# 7. Multi-Sentence Postprocessing\n# -------------------------------------------------------------------------\ndef tokens_to_multisentence(token_ids, vocab, max_sentences=3):\n    sentences = []\n    current_sentence = []\n    end_token_id = vocab.stoi[\"<end>\"]\n\n    for token_id in token_ids:\n        if token_id == end_token_id:\n            if current_sentence:\n                sentences.append(\" \".join(current_sentence))\n            break\n\n        word = vocab.itos.get(token_id, \"<unk>\")\n        if word == \"<unk>\":\n            word = \"something\"\n\n        current_sentence.append(word)\n\n        if word == \".\":\n            sentences.append(\" \".join(current_sentence))\n            current_sentence = []\n\n        if len(sentences) >= max_sentences:\n            break\n\n    if current_sentence and len(sentences) < max_sentences:\n        sentences.append(\" \".join(current_sentence))\n\n    final_output = \". \".join(sentences)\n    if not final_output.endswith(\".\"):\n        final_output += \".\"\n\n    return final_output\n\n# -------------------------------------------------------------------------\n# 8. Prediction from URL (Beam Search by default)\n# -------------------------------------------------------------------------\ndef predict_caption_from_url(model, url, vocab, device, max_sentences=3, beam_size=3, \n                             decode_method=\"beam_search\"):\n    model.eval()\n\n    # Download image\n    response = requests.get(url)\n    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    # Use the inference transform (no augmentation)\n    image = inference_transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        features = model.encoder(image)\n\n        if decode_method == \"beam_search\":\n            token_ids = model.decoder.sample_beam_search(features, vocab, beam_size=beam_size)\n        else:\n            # top-k sampling (k=5, temperature=1.0) as example\n            token_ids = model.decoder.sample_topk(features, vocab, k=5, temperature=1.0)\n\n    caption_text = tokens_to_multisentence(token_ids, vocab, max_sentences=max_sentences)\n    return caption_text\n\n# -------------------------------------------------------------------------\n# 9. Execution: Build, Train, Predict\n# -------------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 9.1: Build the vocabulary\ncaptions_list = df['caption'].tolist()\nvocab = Vocabulary(freq_threshold)\nvocab.build_vocabulary(captions_list)\nprint(\"Vocabulary size:\", len(vocab))\n\n# 9.2: Dataset & DataLoader\n#     We'll use data augmentations for the training transform\ndataset = CaptionDatasetFromDF(df, img_dir, vocab, transform=train_transform, max_seq_length=max_seq_length)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\n# 9.3: Instantiate model\nencoder = EncoderCNN(embed_size).to(device)\ndecoder = DecoderGRU(embed_size, hidden_size, len(vocab), dropout=0.3, num_layers=num_layers, max_seq_length=max_seq_length).to(device)\nmodel   = ImageCaptioningModel(encoder, decoder).to(device)\n\n# 9.4: Loss & Optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# 9.5: Training\nprint(\"Starting training ...\")\nfor epoch in range(num_epochs):\n    epoch_loss = train(model, dataloader, criterion, optimizer, device)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n\n# 9.6: Save the model\ntorch.save(model.state_dict(), \"image_captioning_mobilenetv3_gru.pth\")\nprint(\"Model saved!\")\n\n# 9.7: Inference\ntest_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(\n    model, \n    test_url, \n    vocab, \n    device, \n    max_sentences=3, \n    beam_size=3, \n    decode_method=\"beam_search\"  # or \"topk\" for top-k sampling\n)\nprint(\"Predicted Multi-Sentence Caption:\", predicted_caption)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T00:58:18.490631Z","iopub.execute_input":"2025-04-04T00:58:18.490974Z","iopub.status.idle":"2025-04-04T01:46:31.414931Z","shell.execute_reply.started":"2025-04-04T00:58:18.490951Z","shell.execute_reply":"2025-04-04T01:46:31.413705Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nNumber of captions: 40455\nVocabulary size: 2984\nStarting training ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15], Loss: 2.9992\nEpoch [2/15], Loss: 2.4396\nEpoch [3/15], Loss: 2.1936\nEpoch [4/15], Loss: 2.0047\nEpoch [5/15], Loss: 1.8498\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c630743c5a2c>\u001b[0m in \u001b[0;36m<cell line: 421>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-c630743c5a2c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0moutputs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# (batch, seq_len, vocab_size) vs. (batch, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-c630743c5a2c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, captions)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# Insert the image features at t=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# drop first time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             result = _VF.gru(\n\u001b[0m\u001b[1;32m   1393\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"# Vit Gpt2","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"comp646\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T00:08:09.850828Z","iopub.execute_input":"2025-04-17T00:08:09.851123Z","iopub.status.idle":"2025-04-17T00:08:09.909334Z","shell.execute_reply.started":"2025-04-17T00:08:09.851101Z","shell.execute_reply":"2025-04-17T00:08:09.908359Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip uninstall -y peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T00:08:11.662738Z","iopub.execute_input":"2025-04-17T00:08:11.663110Z","iopub.status.idle":"2025-04-17T00:08:12.939666Z","shell.execute_reply.started":"2025-04-17T00:08:11.663085Z","shell.execute_reply":"2025-04-17T00:08:12.938802Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: peft 0.14.0\nUninstalling peft-0.14.0:\n  Successfully uninstalled peft-0.14.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip uninstall -y transformers\n!pip install --no-cache-dir transformers==4.31.0 accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T00:08:12.940819Z","iopub.execute_input":"2025-04-17T00:08:12.941050Z","iopub.status.idle":"2025-04-17T00:08:25.729820Z","shell.execute_reply.started":"2025-04-17T00:08:12.941030Z","shell.execute_reply":"2025-04-17T00:08:25.728617Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.47.0\nUninstalling transformers-4.47.0:\n  Successfully uninstalled transformers-4.47.0\nCollecting transformers==4.31.0\n  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.32.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.31.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.31.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.31.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.31.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.31.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.31.0) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.31.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.31.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m250.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.31.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#####################################\n# 0. Clean Up & Install (Optional)\n#####################################\n# If you installed \"peft\" or older versions that conflict, remove them:\n# !pip uninstall -y peft transformers\n# Reinstall Transformers 4.31.0 (or similar) + Accelerate\n# !pip install --no-cache-dir transformers==4.31.0 accelerate\n\n#####################################\n# 1. Imports and Setup\n#####################################\nimport os\nimport ssl\nimport requests\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom io import BytesIO\n\n# Optional if you run into SSL issues\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Hugging Face Transformers\nfrom transformers import (\n    VisionEncoderDecoderModel,\n    ViTImageProcessor,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    default_data_collator,\n    set_seed,\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nset_seed(42)\n\n#####################################\n# 2. Config & Paths\n#####################################\ncsv_file = \"/kaggle/input/flickr8k/captions.txt\"\nimg_dir  = \"/kaggle/input/flickr8k/Images\"\n\nencoder_model_name = \"google/vit-base-patch16-224-in21k\"\ndecoder_model_name = \"gpt2\"\n\n# Updated hyperparameters\nEPOCHS              = 3\nBATCH_SIZE          = 2  # reduce from 8 to 2\nLEARNING_RATE       = 5e-5\nMAX_SEQ_LEN         = 32\nFREEZE_ENCODER      = True   # freeze ViT\nUSE_GRAD_CHECKPOINT = False  # set True if you need more memory savings\nUSE_FP16            = True   # half precision -> significantly lowers memory usage\n\n#####################################\n# 3. Load the Captions CSV\n#####################################\ndf = pd.read_csv(csv_file)\nprint(\"Number of captions:\", len(df))\nprint(df.head())\n\n#####################################\n# 4. Image Processor & Tokenizer\n#####################################\nfeature_extractor = ViTImageProcessor.from_pretrained(encoder_model_name)\ntokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n#####################################\n# 5. Dataset\n#####################################\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, dataframe, img_dir, feature_extractor, tokenizer, max_target_length=32, transforms=None):\n        self.df = dataframe.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.feature_extractor = feature_extractor\n        self.tokenizer = tokenizer\n        self.max_target_length = max_target_length\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_name = row[\"image\"]\n        caption  = str(row[\"caption\"])\n\n        # load image\n        path = os.path.join(self.img_dir, img_name)\n        with Image.open(path).convert(\"RGB\") as image:\n            if self.transforms is not None:\n                image = self.transforms(image)\n\n        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values.squeeze()\n\n        # tokenize caption\n        labels = self.tokenizer(\n            caption,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_target_length\n        ).input_ids\n\n        # replace pad_token_id with -100\n        labels = [(lbl if lbl != self.tokenizer.pad_token_id else -100) for lbl in labels]\n        \n        return {\n            \"pixel_values\": pixel_values,\n            \"labels\": torch.tensor(labels, dtype=torch.long),\n        }\n\n# Basic transform: resize to 224x224\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n])\n\n# create dataset\ndataset = ImageCaptionDataset(\n    df,\n    img_dir,\n    feature_extractor=feature_extractor,\n    tokenizer=tokenizer,\n    max_target_length=MAX_SEQ_LEN,\n    transforms=train_transforms\n)\n\n# small train-val split\ntrain_size = int(0.95 * len(dataset))\nval_size   = len(dataset) - train_size\ntrain_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\nprint(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n\n#####################################\n# 6. ViT + GPT-2 Model\n#####################################\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n    encoder_model_name, \n    decoder_model_name\n)\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.eos_token_id           = tokenizer.eos_token_id\nmodel.config.pad_token_id           = tokenizer.pad_token_id\n\n# Generation settings\nmodel.config.max_length           = MAX_SEQ_LEN\nmodel.config.early_stopping       = True\nmodel.config.no_repeat_ngram_size = 2\n\n# (Optional) Freeze the entire ViT encoder to save memory\nif FREEZE_ENCODER:\n    for param in model.encoder.parameters():\n        param.requires_grad = False\n\n# (Optional) Gradient checkpointing\nmodel.decoder.gradient_checkpointing = USE_GRAD_CHECKPOINT\n\nmodel.to(device)\n\n#####################################\n# 7. Seq2SeqTrainer Setup\n#####################################\ndef collate_fn(batch):\n    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n    labels       = torch.stack([item[\"labels\"] for item in batch])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"vit-gpt2-checkpoints\",\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    # Turn off evaluation to prevent OOM mid-training\n    evaluation_strategy=\"no\",\n    # Turn off checkpoint saving to reduce overhead\n    save_strategy=\"no\",\n    num_train_epochs=EPOCHS,\n    learning_rate=LEARNING_RATE,\n    logging_steps=100,\n    report_to=\"none\",\n    push_to_hub=False,\n    fp16=USE_FP16,                        # mixed precision\n    gradient_checkpointing=USE_GRAD_CHECKPOINT,  # or keep the code here as well\n)\n\n# minimal dummy metric\ndef compute_metrics(eval_pred):\n    return {\"dummy_metric\": 0.0}\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,  # won't run since evaluation_strategy=\"no\"\n    data_collator=collate_fn,\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics,\n)\n\n#####################################\n# 8. Train\n#####################################\ntrainer.train()\n\n#####################################\n# 9. Inference from a URL\n#####################################\ndef predict_caption_from_url(url, model, feature_extractor, tokenizer, device,\n                             max_length=32, num_beams=4):\n    model.eval()\n    response = requests.get(url)\n    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n    pixel_values = feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n    with torch.no_grad():\n        output_ids = model.generate(\n            pixel_values,\n            max_length=max_length,\n            num_beams=num_beams,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id\n        )\n\n    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return caption\n\n# Example inference\ntest_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(test_url, model, feature_extractor, tokenizer, device)\nprint(\"Predicted Caption:\", predicted_caption)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:28:57.504747Z","iopub.execute_input":"2025-04-04T02:28:57.505075Z","iopub.status.idle":"2025-04-04T04:15:09.800584Z","shell.execute_reply.started":"2025-04-04T02:28:57.505050Z","shell.execute_reply":"2025-04-04T04:15:09.798953Z"}},"outputs":[{"name":"stdout","text":"Number of captions: 40455\n                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e3f03e7d5904725b6a45b7bc8bc4fea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2334acb27f474124b202f681c347cebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15765b0ec22d425182078544886b6c48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dd6b4c9213847bba75325becf2a7edd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9dd960ffd6b4918aaa6ec8c3b55879f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07706cbadd6e48bd9c0ca4fd136e0c60"}},"metadata":{}},{"name":"stdout","text":"Train size: 38432 Val size: 2023\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3105c428553460a878a221bddc2ce64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c705f936b7544c8a2e2bd2ac72882bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d939aca5eeb5443cb5ad609269c26d58"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d81ff6766194e738ec27171a0d1944b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-2-9b9d36a93df3>:193: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='57648' max='57648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [57648/57648 1:45:11, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.767600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.408300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.310100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.281100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.265200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.333400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.278600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.235000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.040100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.160200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.016700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.014900</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.056800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.991300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.986300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.970300</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>3.058400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>3.020200</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.934500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.827100</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.784700</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.937000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.902500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.921200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.845500</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>2.805500</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>2.851900</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>2.819000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>2.839800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.766600</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>2.758600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>2.853000</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>2.801700</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>2.774700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>2.668700</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>2.848300</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>2.853300</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>2.700400</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>2.775000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.662300</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>2.843500</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>2.694100</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>2.773400</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>2.666600</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>2.792900</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>2.857400</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>2.649200</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>2.649500</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>2.754000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.641500</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>2.699200</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>2.597400</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>2.719400</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>2.720500</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>2.861700</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>2.617500</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>2.718800</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>2.673900</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>2.626100</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.618300</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>2.638600</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>2.658200</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>2.768000</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>2.644700</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>2.799700</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>2.601800</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>2.610700</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>2.711800</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>2.625700</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>2.637300</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>2.666000</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>2.719600</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>2.800000</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>2.653700</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>2.662600</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>2.710500</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>2.729100</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>2.571500</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>2.642100</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>2.650900</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>2.625900</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>2.567300</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>2.669500</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>2.574000</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>2.481900</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>2.622200</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>2.628800</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>2.571700</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>2.637700</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>2.564000</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>2.505800</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>2.518000</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>2.524400</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>2.689100</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>2.690700</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>2.629600</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>2.684100</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>2.644100</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>2.584300</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>2.516200</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>2.554100</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>2.551400</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>2.550700</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>2.639200</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>2.604000</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>2.597900</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>2.631800</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>2.538700</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>2.461000</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>2.658200</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>2.616100</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>2.573900</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>2.541400</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>2.610900</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>2.583400</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>2.525400</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>2.596300</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>2.598400</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>2.574900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>2.503100</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>2.461900</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>2.399000</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>2.538000</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>2.539800</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>2.479500</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>2.440900</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>2.703800</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>2.524800</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>2.402800</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>2.650200</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>2.426600</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>2.510100</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>2.531800</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>2.428400</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>2.495500</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>2.561200</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>2.618800</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>2.619200</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>2.634500</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>2.363700</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>2.573300</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>2.471300</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>2.428400</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>2.554800</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>2.436200</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>2.494000</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>2.481000</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>2.447900</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>2.436300</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>2.676300</td>\n    </tr>\n    <tr>\n      <td>15100</td>\n      <td>2.533400</td>\n    </tr>\n    <tr>\n      <td>15200</td>\n      <td>2.491900</td>\n    </tr>\n    <tr>\n      <td>15300</td>\n      <td>2.447200</td>\n    </tr>\n    <tr>\n      <td>15400</td>\n      <td>2.522700</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>2.547300</td>\n    </tr>\n    <tr>\n      <td>15600</td>\n      <td>2.496100</td>\n    </tr>\n    <tr>\n      <td>15700</td>\n      <td>2.455600</td>\n    </tr>\n    <tr>\n      <td>15800</td>\n      <td>2.553400</td>\n    </tr>\n    <tr>\n      <td>15900</td>\n      <td>2.528600</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>2.388200</td>\n    </tr>\n    <tr>\n      <td>16100</td>\n      <td>2.388700</td>\n    </tr>\n    <tr>\n      <td>16200</td>\n      <td>2.445500</td>\n    </tr>\n    <tr>\n      <td>16300</td>\n      <td>2.607600</td>\n    </tr>\n    <tr>\n      <td>16400</td>\n      <td>2.437300</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>2.483300</td>\n    </tr>\n    <tr>\n      <td>16600</td>\n      <td>2.422300</td>\n    </tr>\n    <tr>\n      <td>16700</td>\n      <td>2.494300</td>\n    </tr>\n    <tr>\n      <td>16800</td>\n      <td>2.478800</td>\n    </tr>\n    <tr>\n      <td>16900</td>\n      <td>2.441500</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>2.411700</td>\n    </tr>\n    <tr>\n      <td>17100</td>\n      <td>2.417600</td>\n    </tr>\n    <tr>\n      <td>17200</td>\n      <td>2.516200</td>\n    </tr>\n    <tr>\n      <td>17300</td>\n      <td>2.586200</td>\n    </tr>\n    <tr>\n      <td>17400</td>\n      <td>2.538900</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>2.480200</td>\n    </tr>\n    <tr>\n      <td>17600</td>\n      <td>2.455100</td>\n    </tr>\n    <tr>\n      <td>17700</td>\n      <td>2.533200</td>\n    </tr>\n    <tr>\n      <td>17800</td>\n      <td>2.571400</td>\n    </tr>\n    <tr>\n      <td>17900</td>\n      <td>2.416600</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>2.570700</td>\n    </tr>\n    <tr>\n      <td>18100</td>\n      <td>2.432700</td>\n    </tr>\n    <tr>\n      <td>18200</td>\n      <td>2.437100</td>\n    </tr>\n    <tr>\n      <td>18300</td>\n      <td>2.447600</td>\n    </tr>\n    <tr>\n      <td>18400</td>\n      <td>2.417700</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>2.472600</td>\n    </tr>\n    <tr>\n      <td>18600</td>\n      <td>2.384800</td>\n    </tr>\n    <tr>\n      <td>18700</td>\n      <td>2.361700</td>\n    </tr>\n    <tr>\n      <td>18800</td>\n      <td>2.455500</td>\n    </tr>\n    <tr>\n      <td>18900</td>\n      <td>2.442300</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>2.280400</td>\n    </tr>\n    <tr>\n      <td>19100</td>\n      <td>2.439600</td>\n    </tr>\n    <tr>\n      <td>19200</td>\n      <td>2.466500</td>\n    </tr>\n    <tr>\n      <td>19300</td>\n      <td>2.286700</td>\n    </tr>\n    <tr>\n      <td>19400</td>\n      <td>2.087600</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>2.224000</td>\n    </tr>\n    <tr>\n      <td>19600</td>\n      <td>2.184800</td>\n    </tr>\n    <tr>\n      <td>19700</td>\n      <td>2.162600</td>\n    </tr>\n    <tr>\n      <td>19800</td>\n      <td>2.184800</td>\n    </tr>\n    <tr>\n      <td>19900</td>\n      <td>2.155600</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>2.245700</td>\n    </tr>\n    <tr>\n      <td>20100</td>\n      <td>2.138300</td>\n    </tr>\n    <tr>\n      <td>20200</td>\n      <td>2.235200</td>\n    </tr>\n    <tr>\n      <td>20300</td>\n      <td>2.107400</td>\n    </tr>\n    <tr>\n      <td>20400</td>\n      <td>2.270800</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>2.182900</td>\n    </tr>\n    <tr>\n      <td>20600</td>\n      <td>2.128900</td>\n    </tr>\n    <tr>\n      <td>20700</td>\n      <td>2.213500</td>\n    </tr>\n    <tr>\n      <td>20800</td>\n      <td>2.209300</td>\n    </tr>\n    <tr>\n      <td>20900</td>\n      <td>2.177100</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>2.190100</td>\n    </tr>\n    <tr>\n      <td>21100</td>\n      <td>2.228200</td>\n    </tr>\n    <tr>\n      <td>21200</td>\n      <td>2.109000</td>\n    </tr>\n    <tr>\n      <td>21300</td>\n      <td>2.101400</td>\n    </tr>\n    <tr>\n      <td>21400</td>\n      <td>2.185800</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>2.247500</td>\n    </tr>\n    <tr>\n      <td>21600</td>\n      <td>2.259700</td>\n    </tr>\n    <tr>\n      <td>21700</td>\n      <td>2.268700</td>\n    </tr>\n    <tr>\n      <td>21800</td>\n      <td>2.212700</td>\n    </tr>\n    <tr>\n      <td>21900</td>\n      <td>2.225700</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>2.159800</td>\n    </tr>\n    <tr>\n      <td>22100</td>\n      <td>2.203700</td>\n    </tr>\n    <tr>\n      <td>22200</td>\n      <td>2.156300</td>\n    </tr>\n    <tr>\n      <td>22300</td>\n      <td>2.317800</td>\n    </tr>\n    <tr>\n      <td>22400</td>\n      <td>2.175500</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>2.187800</td>\n    </tr>\n    <tr>\n      <td>22600</td>\n      <td>2.118200</td>\n    </tr>\n    <tr>\n      <td>22700</td>\n      <td>2.340800</td>\n    </tr>\n    <tr>\n      <td>22800</td>\n      <td>2.208900</td>\n    </tr>\n    <tr>\n      <td>22900</td>\n      <td>2.167300</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>2.061100</td>\n    </tr>\n    <tr>\n      <td>23100</td>\n      <td>2.264900</td>\n    </tr>\n    <tr>\n      <td>23200</td>\n      <td>2.067800</td>\n    </tr>\n    <tr>\n      <td>23300</td>\n      <td>2.247000</td>\n    </tr>\n    <tr>\n      <td>23400</td>\n      <td>2.172100</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>2.190400</td>\n    </tr>\n    <tr>\n      <td>23600</td>\n      <td>2.230400</td>\n    </tr>\n    <tr>\n      <td>23700</td>\n      <td>2.109000</td>\n    </tr>\n    <tr>\n      <td>23800</td>\n      <td>2.201600</td>\n    </tr>\n    <tr>\n      <td>23900</td>\n      <td>2.192600</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>2.165500</td>\n    </tr>\n    <tr>\n      <td>24100</td>\n      <td>2.205600</td>\n    </tr>\n    <tr>\n      <td>24200</td>\n      <td>2.193400</td>\n    </tr>\n    <tr>\n      <td>24300</td>\n      <td>2.177100</td>\n    </tr>\n    <tr>\n      <td>24400</td>\n      <td>2.163100</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>2.143900</td>\n    </tr>\n    <tr>\n      <td>24600</td>\n      <td>2.120700</td>\n    </tr>\n    <tr>\n      <td>24700</td>\n      <td>2.257300</td>\n    </tr>\n    <tr>\n      <td>24800</td>\n      <td>2.216200</td>\n    </tr>\n    <tr>\n      <td>24900</td>\n      <td>2.189500</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>2.114200</td>\n    </tr>\n    <tr>\n      <td>25100</td>\n      <td>2.143700</td>\n    </tr>\n    <tr>\n      <td>25200</td>\n      <td>2.170900</td>\n    </tr>\n    <tr>\n      <td>25300</td>\n      <td>2.203000</td>\n    </tr>\n    <tr>\n      <td>25400</td>\n      <td>2.268900</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>2.256500</td>\n    </tr>\n    <tr>\n      <td>25600</td>\n      <td>2.252100</td>\n    </tr>\n    <tr>\n      <td>25700</td>\n      <td>2.162200</td>\n    </tr>\n    <tr>\n      <td>25800</td>\n      <td>2.182400</td>\n    </tr>\n    <tr>\n      <td>25900</td>\n      <td>2.114100</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>2.163000</td>\n    </tr>\n    <tr>\n      <td>26100</td>\n      <td>2.156500</td>\n    </tr>\n    <tr>\n      <td>26200</td>\n      <td>2.082300</td>\n    </tr>\n    <tr>\n      <td>26300</td>\n      <td>2.198700</td>\n    </tr>\n    <tr>\n      <td>26400</td>\n      <td>2.202500</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>2.155800</td>\n    </tr>\n    <tr>\n      <td>26600</td>\n      <td>2.138400</td>\n    </tr>\n    <tr>\n      <td>26700</td>\n      <td>2.224500</td>\n    </tr>\n    <tr>\n      <td>26800</td>\n      <td>2.210200</td>\n    </tr>\n    <tr>\n      <td>26900</td>\n      <td>2.122800</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>2.188500</td>\n    </tr>\n    <tr>\n      <td>27100</td>\n      <td>2.228000</td>\n    </tr>\n    <tr>\n      <td>27200</td>\n      <td>2.132500</td>\n    </tr>\n    <tr>\n      <td>27300</td>\n      <td>2.148400</td>\n    </tr>\n    <tr>\n      <td>27400</td>\n      <td>2.147200</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>2.165200</td>\n    </tr>\n    <tr>\n      <td>27600</td>\n      <td>2.119000</td>\n    </tr>\n    <tr>\n      <td>27700</td>\n      <td>2.255600</td>\n    </tr>\n    <tr>\n      <td>27800</td>\n      <td>2.230100</td>\n    </tr>\n    <tr>\n      <td>27900</td>\n      <td>2.153000</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>2.073200</td>\n    </tr>\n    <tr>\n      <td>28100</td>\n      <td>2.118000</td>\n    </tr>\n    <tr>\n      <td>28200</td>\n      <td>2.200100</td>\n    </tr>\n    <tr>\n      <td>28300</td>\n      <td>2.153900</td>\n    </tr>\n    <tr>\n      <td>28400</td>\n      <td>2.152100</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>2.203600</td>\n    </tr>\n    <tr>\n      <td>28600</td>\n      <td>2.163800</td>\n    </tr>\n    <tr>\n      <td>28700</td>\n      <td>2.238100</td>\n    </tr>\n    <tr>\n      <td>28800</td>\n      <td>2.131100</td>\n    </tr>\n    <tr>\n      <td>28900</td>\n      <td>2.159300</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>2.207600</td>\n    </tr>\n    <tr>\n      <td>29100</td>\n      <td>2.148300</td>\n    </tr>\n    <tr>\n      <td>29200</td>\n      <td>2.153000</td>\n    </tr>\n    <tr>\n      <td>29300</td>\n      <td>2.161600</td>\n    </tr>\n    <tr>\n      <td>29400</td>\n      <td>2.196200</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>2.161100</td>\n    </tr>\n    <tr>\n      <td>29600</td>\n      <td>2.131300</td>\n    </tr>\n    <tr>\n      <td>29700</td>\n      <td>2.135100</td>\n    </tr>\n    <tr>\n      <td>29800</td>\n      <td>2.036300</td>\n    </tr>\n    <tr>\n      <td>29900</td>\n      <td>2.139200</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>2.222200</td>\n    </tr>\n    <tr>\n      <td>30100</td>\n      <td>2.223300</td>\n    </tr>\n    <tr>\n      <td>30200</td>\n      <td>2.264000</td>\n    </tr>\n    <tr>\n      <td>30300</td>\n      <td>2.139800</td>\n    </tr>\n    <tr>\n      <td>30400</td>\n      <td>2.159900</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>2.265500</td>\n    </tr>\n    <tr>\n      <td>30600</td>\n      <td>2.202800</td>\n    </tr>\n    <tr>\n      <td>30700</td>\n      <td>2.154900</td>\n    </tr>\n    <tr>\n      <td>30800</td>\n      <td>2.175600</td>\n    </tr>\n    <tr>\n      <td>30900</td>\n      <td>2.106800</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>2.209600</td>\n    </tr>\n    <tr>\n      <td>31100</td>\n      <td>2.240900</td>\n    </tr>\n    <tr>\n      <td>31200</td>\n      <td>2.173700</td>\n    </tr>\n    <tr>\n      <td>31300</td>\n      <td>2.064900</td>\n    </tr>\n    <tr>\n      <td>31400</td>\n      <td>2.200100</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>2.134900</td>\n    </tr>\n    <tr>\n      <td>31600</td>\n      <td>2.188700</td>\n    </tr>\n    <tr>\n      <td>31700</td>\n      <td>2.139800</td>\n    </tr>\n    <tr>\n      <td>31800</td>\n      <td>2.128700</td>\n    </tr>\n    <tr>\n      <td>31900</td>\n      <td>2.197200</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>2.056200</td>\n    </tr>\n    <tr>\n      <td>32100</td>\n      <td>2.184100</td>\n    </tr>\n    <tr>\n      <td>32200</td>\n      <td>2.145800</td>\n    </tr>\n    <tr>\n      <td>32300</td>\n      <td>2.183100</td>\n    </tr>\n    <tr>\n      <td>32400</td>\n      <td>2.121800</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>2.195900</td>\n    </tr>\n    <tr>\n      <td>32600</td>\n      <td>2.159300</td>\n    </tr>\n    <tr>\n      <td>32700</td>\n      <td>2.164100</td>\n    </tr>\n    <tr>\n      <td>32800</td>\n      <td>2.125700</td>\n    </tr>\n    <tr>\n      <td>32900</td>\n      <td>2.194100</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>2.158900</td>\n    </tr>\n    <tr>\n      <td>33100</td>\n      <td>2.114000</td>\n    </tr>\n    <tr>\n      <td>33200</td>\n      <td>2.166900</td>\n    </tr>\n    <tr>\n      <td>33300</td>\n      <td>2.185500</td>\n    </tr>\n    <tr>\n      <td>33400</td>\n      <td>2.124000</td>\n    </tr>\n    <tr>\n      <td>33500</td>\n      <td>2.113200</td>\n    </tr>\n    <tr>\n      <td>33600</td>\n      <td>2.170300</td>\n    </tr>\n    <tr>\n      <td>33700</td>\n      <td>2.159600</td>\n    </tr>\n    <tr>\n      <td>33800</td>\n      <td>2.261300</td>\n    </tr>\n    <tr>\n      <td>33900</td>\n      <td>2.173000</td>\n    </tr>\n    <tr>\n      <td>34000</td>\n      <td>2.058600</td>\n    </tr>\n    <tr>\n      <td>34100</td>\n      <td>2.178300</td>\n    </tr>\n    <tr>\n      <td>34200</td>\n      <td>2.107200</td>\n    </tr>\n    <tr>\n      <td>34300</td>\n      <td>2.148300</td>\n    </tr>\n    <tr>\n      <td>34400</td>\n      <td>2.184000</td>\n    </tr>\n    <tr>\n      <td>34500</td>\n      <td>2.152300</td>\n    </tr>\n    <tr>\n      <td>34600</td>\n      <td>2.145500</td>\n    </tr>\n    <tr>\n      <td>34700</td>\n      <td>2.227000</td>\n    </tr>\n    <tr>\n      <td>34800</td>\n      <td>2.181800</td>\n    </tr>\n    <tr>\n      <td>34900</td>\n      <td>2.201200</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>2.025400</td>\n    </tr>\n    <tr>\n      <td>35100</td>\n      <td>2.122100</td>\n    </tr>\n    <tr>\n      <td>35200</td>\n      <td>2.168100</td>\n    </tr>\n    <tr>\n      <td>35300</td>\n      <td>2.127600</td>\n    </tr>\n    <tr>\n      <td>35400</td>\n      <td>2.193200</td>\n    </tr>\n    <tr>\n      <td>35500</td>\n      <td>2.181000</td>\n    </tr>\n    <tr>\n      <td>35600</td>\n      <td>2.187500</td>\n    </tr>\n    <tr>\n      <td>35700</td>\n      <td>2.115500</td>\n    </tr>\n    <tr>\n      <td>35800</td>\n      <td>2.131800</td>\n    </tr>\n    <tr>\n      <td>35900</td>\n      <td>2.072400</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>2.112300</td>\n    </tr>\n    <tr>\n      <td>36100</td>\n      <td>2.145100</td>\n    </tr>\n    <tr>\n      <td>36200</td>\n      <td>2.152600</td>\n    </tr>\n    <tr>\n      <td>36300</td>\n      <td>2.197600</td>\n    </tr>\n    <tr>\n      <td>36400</td>\n      <td>2.123600</td>\n    </tr>\n    <tr>\n      <td>36500</td>\n      <td>2.114200</td>\n    </tr>\n    <tr>\n      <td>36600</td>\n      <td>2.148200</td>\n    </tr>\n    <tr>\n      <td>36700</td>\n      <td>2.159300</td>\n    </tr>\n    <tr>\n      <td>36800</td>\n      <td>2.192300</td>\n    </tr>\n    <tr>\n      <td>36900</td>\n      <td>2.139700</td>\n    </tr>\n    <tr>\n      <td>37000</td>\n      <td>2.133200</td>\n    </tr>\n    <tr>\n      <td>37100</td>\n      <td>2.090100</td>\n    </tr>\n    <tr>\n      <td>37200</td>\n      <td>2.071100</td>\n    </tr>\n    <tr>\n      <td>37300</td>\n      <td>2.177500</td>\n    </tr>\n    <tr>\n      <td>37400</td>\n      <td>2.157500</td>\n    </tr>\n    <tr>\n      <td>37500</td>\n      <td>2.160300</td>\n    </tr>\n    <tr>\n      <td>37600</td>\n      <td>2.205800</td>\n    </tr>\n    <tr>\n      <td>37700</td>\n      <td>2.173300</td>\n    </tr>\n    <tr>\n      <td>37800</td>\n      <td>2.050700</td>\n    </tr>\n    <tr>\n      <td>37900</td>\n      <td>2.218800</td>\n    </tr>\n    <tr>\n      <td>38000</td>\n      <td>2.108300</td>\n    </tr>\n    <tr>\n      <td>38100</td>\n      <td>2.012500</td>\n    </tr>\n    <tr>\n      <td>38200</td>\n      <td>2.083200</td>\n    </tr>\n    <tr>\n      <td>38300</td>\n      <td>2.129100</td>\n    </tr>\n    <tr>\n      <td>38400</td>\n      <td>2.158700</td>\n    </tr>\n    <tr>\n      <td>38500</td>\n      <td>1.972200</td>\n    </tr>\n    <tr>\n      <td>38600</td>\n      <td>1.918800</td>\n    </tr>\n    <tr>\n      <td>38700</td>\n      <td>1.866400</td>\n    </tr>\n    <tr>\n      <td>38800</td>\n      <td>1.893000</td>\n    </tr>\n    <tr>\n      <td>38900</td>\n      <td>1.879400</td>\n    </tr>\n    <tr>\n      <td>39000</td>\n      <td>1.916300</td>\n    </tr>\n    <tr>\n      <td>39100</td>\n      <td>1.850400</td>\n    </tr>\n    <tr>\n      <td>39200</td>\n      <td>1.869100</td>\n    </tr>\n    <tr>\n      <td>39300</td>\n      <td>1.957400</td>\n    </tr>\n    <tr>\n      <td>39400</td>\n      <td>1.897000</td>\n    </tr>\n    <tr>\n      <td>39500</td>\n      <td>1.821500</td>\n    </tr>\n    <tr>\n      <td>39600</td>\n      <td>1.910100</td>\n    </tr>\n    <tr>\n      <td>39700</td>\n      <td>1.850300</td>\n    </tr>\n    <tr>\n      <td>39800</td>\n      <td>1.948700</td>\n    </tr>\n    <tr>\n      <td>39900</td>\n      <td>1.956700</td>\n    </tr>\n    <tr>\n      <td>40000</td>\n      <td>1.861100</td>\n    </tr>\n    <tr>\n      <td>40100</td>\n      <td>1.865600</td>\n    </tr>\n    <tr>\n      <td>40200</td>\n      <td>1.930800</td>\n    </tr>\n    <tr>\n      <td>40300</td>\n      <td>1.902500</td>\n    </tr>\n    <tr>\n      <td>40400</td>\n      <td>1.840000</td>\n    </tr>\n    <tr>\n      <td>40500</td>\n      <td>1.944900</td>\n    </tr>\n    <tr>\n      <td>40600</td>\n      <td>1.915800</td>\n    </tr>\n    <tr>\n      <td>40700</td>\n      <td>1.872700</td>\n    </tr>\n    <tr>\n      <td>40800</td>\n      <td>1.898800</td>\n    </tr>\n    <tr>\n      <td>40900</td>\n      <td>1.928300</td>\n    </tr>\n    <tr>\n      <td>41000</td>\n      <td>1.853200</td>\n    </tr>\n    <tr>\n      <td>41100</td>\n      <td>1.969600</td>\n    </tr>\n    <tr>\n      <td>41200</td>\n      <td>1.905700</td>\n    </tr>\n    <tr>\n      <td>41300</td>\n      <td>1.840300</td>\n    </tr>\n    <tr>\n      <td>41400</td>\n      <td>1.874000</td>\n    </tr>\n    <tr>\n      <td>41500</td>\n      <td>1.864600</td>\n    </tr>\n    <tr>\n      <td>41600</td>\n      <td>2.018300</td>\n    </tr>\n    <tr>\n      <td>41700</td>\n      <td>1.930600</td>\n    </tr>\n    <tr>\n      <td>41800</td>\n      <td>1.846900</td>\n    </tr>\n    <tr>\n      <td>41900</td>\n      <td>1.963200</td>\n    </tr>\n    <tr>\n      <td>42000</td>\n      <td>1.886200</td>\n    </tr>\n    <tr>\n      <td>42100</td>\n      <td>1.934400</td>\n    </tr>\n    <tr>\n      <td>42200</td>\n      <td>1.943800</td>\n    </tr>\n    <tr>\n      <td>42300</td>\n      <td>1.904700</td>\n    </tr>\n    <tr>\n      <td>42400</td>\n      <td>1.957900</td>\n    </tr>\n    <tr>\n      <td>42500</td>\n      <td>1.904500</td>\n    </tr>\n    <tr>\n      <td>42600</td>\n      <td>1.932300</td>\n    </tr>\n    <tr>\n      <td>42700</td>\n      <td>1.897200</td>\n    </tr>\n    <tr>\n      <td>42800</td>\n      <td>1.944300</td>\n    </tr>\n    <tr>\n      <td>42900</td>\n      <td>1.938300</td>\n    </tr>\n    <tr>\n      <td>43000</td>\n      <td>1.896800</td>\n    </tr>\n    <tr>\n      <td>43100</td>\n      <td>1.921100</td>\n    </tr>\n    <tr>\n      <td>43200</td>\n      <td>1.924400</td>\n    </tr>\n    <tr>\n      <td>43300</td>\n      <td>1.905400</td>\n    </tr>\n    <tr>\n      <td>43400</td>\n      <td>1.789700</td>\n    </tr>\n    <tr>\n      <td>43500</td>\n      <td>1.932800</td>\n    </tr>\n    <tr>\n      <td>43600</td>\n      <td>1.779500</td>\n    </tr>\n    <tr>\n      <td>43700</td>\n      <td>1.848700</td>\n    </tr>\n    <tr>\n      <td>43800</td>\n      <td>1.997200</td>\n    </tr>\n    <tr>\n      <td>43900</td>\n      <td>1.926200</td>\n    </tr>\n    <tr>\n      <td>44000</td>\n      <td>1.785600</td>\n    </tr>\n    <tr>\n      <td>44100</td>\n      <td>1.930300</td>\n    </tr>\n    <tr>\n      <td>44200</td>\n      <td>1.851200</td>\n    </tr>\n    <tr>\n      <td>44300</td>\n      <td>1.857700</td>\n    </tr>\n    <tr>\n      <td>44400</td>\n      <td>1.864300</td>\n    </tr>\n    <tr>\n      <td>44500</td>\n      <td>1.930000</td>\n    </tr>\n    <tr>\n      <td>44600</td>\n      <td>1.895500</td>\n    </tr>\n    <tr>\n      <td>44700</td>\n      <td>1.926700</td>\n    </tr>\n    <tr>\n      <td>44800</td>\n      <td>1.924700</td>\n    </tr>\n    <tr>\n      <td>44900</td>\n      <td>1.930400</td>\n    </tr>\n    <tr>\n      <td>45000</td>\n      <td>1.861400</td>\n    </tr>\n    <tr>\n      <td>45100</td>\n      <td>1.944900</td>\n    </tr>\n    <tr>\n      <td>45200</td>\n      <td>1.824800</td>\n    </tr>\n    <tr>\n      <td>45300</td>\n      <td>1.923800</td>\n    </tr>\n    <tr>\n      <td>45400</td>\n      <td>1.932500</td>\n    </tr>\n    <tr>\n      <td>45500</td>\n      <td>1.857700</td>\n    </tr>\n    <tr>\n      <td>45600</td>\n      <td>1.865000</td>\n    </tr>\n    <tr>\n      <td>45700</td>\n      <td>1.890400</td>\n    </tr>\n    <tr>\n      <td>45800</td>\n      <td>1.958700</td>\n    </tr>\n    <tr>\n      <td>45900</td>\n      <td>1.890400</td>\n    </tr>\n    <tr>\n      <td>46000</td>\n      <td>1.875600</td>\n    </tr>\n    <tr>\n      <td>46100</td>\n      <td>1.895200</td>\n    </tr>\n    <tr>\n      <td>46200</td>\n      <td>1.941400</td>\n    </tr>\n    <tr>\n      <td>46300</td>\n      <td>1.874700</td>\n    </tr>\n    <tr>\n      <td>46400</td>\n      <td>1.848500</td>\n    </tr>\n    <tr>\n      <td>46500</td>\n      <td>1.855800</td>\n    </tr>\n    <tr>\n      <td>46600</td>\n      <td>1.924700</td>\n    </tr>\n    <tr>\n      <td>46700</td>\n      <td>1.938900</td>\n    </tr>\n    <tr>\n      <td>46800</td>\n      <td>1.833300</td>\n    </tr>\n    <tr>\n      <td>46900</td>\n      <td>1.906100</td>\n    </tr>\n    <tr>\n      <td>47000</td>\n      <td>1.970000</td>\n    </tr>\n    <tr>\n      <td>47100</td>\n      <td>1.862000</td>\n    </tr>\n    <tr>\n      <td>47200</td>\n      <td>1.895300</td>\n    </tr>\n    <tr>\n      <td>47300</td>\n      <td>2.008100</td>\n    </tr>\n    <tr>\n      <td>47400</td>\n      <td>1.982000</td>\n    </tr>\n    <tr>\n      <td>47500</td>\n      <td>1.837500</td>\n    </tr>\n    <tr>\n      <td>47600</td>\n      <td>1.863700</td>\n    </tr>\n    <tr>\n      <td>47700</td>\n      <td>1.914100</td>\n    </tr>\n    <tr>\n      <td>47800</td>\n      <td>1.899000</td>\n    </tr>\n    <tr>\n      <td>47900</td>\n      <td>1.876300</td>\n    </tr>\n    <tr>\n      <td>48000</td>\n      <td>1.940300</td>\n    </tr>\n    <tr>\n      <td>48100</td>\n      <td>1.838700</td>\n    </tr>\n    <tr>\n      <td>48200</td>\n      <td>1.820700</td>\n    </tr>\n    <tr>\n      <td>48300</td>\n      <td>1.911600</td>\n    </tr>\n    <tr>\n      <td>48400</td>\n      <td>1.910600</td>\n    </tr>\n    <tr>\n      <td>48500</td>\n      <td>1.891800</td>\n    </tr>\n    <tr>\n      <td>48600</td>\n      <td>1.913400</td>\n    </tr>\n    <tr>\n      <td>48700</td>\n      <td>1.871500</td>\n    </tr>\n    <tr>\n      <td>48800</td>\n      <td>1.832900</td>\n    </tr>\n    <tr>\n      <td>48900</td>\n      <td>1.840900</td>\n    </tr>\n    <tr>\n      <td>49000</td>\n      <td>1.932700</td>\n    </tr>\n    <tr>\n      <td>49100</td>\n      <td>1.923200</td>\n    </tr>\n    <tr>\n      <td>49200</td>\n      <td>1.889800</td>\n    </tr>\n    <tr>\n      <td>49300</td>\n      <td>1.834100</td>\n    </tr>\n    <tr>\n      <td>49400</td>\n      <td>1.888900</td>\n    </tr>\n    <tr>\n      <td>49500</td>\n      <td>1.899000</td>\n    </tr>\n    <tr>\n      <td>49600</td>\n      <td>1.849000</td>\n    </tr>\n    <tr>\n      <td>49700</td>\n      <td>1.837700</td>\n    </tr>\n    <tr>\n      <td>49800</td>\n      <td>1.899400</td>\n    </tr>\n    <tr>\n      <td>49900</td>\n      <td>1.901100</td>\n    </tr>\n    <tr>\n      <td>50000</td>\n      <td>1.921100</td>\n    </tr>\n    <tr>\n      <td>50100</td>\n      <td>1.951800</td>\n    </tr>\n    <tr>\n      <td>50200</td>\n      <td>1.860500</td>\n    </tr>\n    <tr>\n      <td>50300</td>\n      <td>1.944300</td>\n    </tr>\n    <tr>\n      <td>50400</td>\n      <td>2.017700</td>\n    </tr>\n    <tr>\n      <td>50500</td>\n      <td>1.902900</td>\n    </tr>\n    <tr>\n      <td>50600</td>\n      <td>1.915400</td>\n    </tr>\n    <tr>\n      <td>50700</td>\n      <td>1.869900</td>\n    </tr>\n    <tr>\n      <td>50800</td>\n      <td>1.784800</td>\n    </tr>\n    <tr>\n      <td>50900</td>\n      <td>1.806900</td>\n    </tr>\n    <tr>\n      <td>51000</td>\n      <td>1.911400</td>\n    </tr>\n    <tr>\n      <td>51100</td>\n      <td>1.957100</td>\n    </tr>\n    <tr>\n      <td>51200</td>\n      <td>1.828600</td>\n    </tr>\n    <tr>\n      <td>51300</td>\n      <td>1.901600</td>\n    </tr>\n    <tr>\n      <td>51400</td>\n      <td>1.967500</td>\n    </tr>\n    <tr>\n      <td>51500</td>\n      <td>1.919800</td>\n    </tr>\n    <tr>\n      <td>51600</td>\n      <td>1.888100</td>\n    </tr>\n    <tr>\n      <td>51700</td>\n      <td>1.938300</td>\n    </tr>\n    <tr>\n      <td>51800</td>\n      <td>1.808300</td>\n    </tr>\n    <tr>\n      <td>51900</td>\n      <td>1.847600</td>\n    </tr>\n    <tr>\n      <td>52000</td>\n      <td>1.876300</td>\n    </tr>\n    <tr>\n      <td>52100</td>\n      <td>1.903000</td>\n    </tr>\n    <tr>\n      <td>52200</td>\n      <td>1.891600</td>\n    </tr>\n    <tr>\n      <td>52300</td>\n      <td>1.875800</td>\n    </tr>\n    <tr>\n      <td>52400</td>\n      <td>1.917700</td>\n    </tr>\n    <tr>\n      <td>52500</td>\n      <td>1.931100</td>\n    </tr>\n    <tr>\n      <td>52600</td>\n      <td>1.910100</td>\n    </tr>\n    <tr>\n      <td>52700</td>\n      <td>1.926200</td>\n    </tr>\n    <tr>\n      <td>52800</td>\n      <td>1.852900</td>\n    </tr>\n    <tr>\n      <td>52900</td>\n      <td>1.967900</td>\n    </tr>\n    <tr>\n      <td>53000</td>\n      <td>1.901500</td>\n    </tr>\n    <tr>\n      <td>53100</td>\n      <td>1.843900</td>\n    </tr>\n    <tr>\n      <td>53200</td>\n      <td>1.872700</td>\n    </tr>\n    <tr>\n      <td>53300</td>\n      <td>1.892300</td>\n    </tr>\n    <tr>\n      <td>53400</td>\n      <td>1.895100</td>\n    </tr>\n    <tr>\n      <td>53500</td>\n      <td>1.865800</td>\n    </tr>\n    <tr>\n      <td>53600</td>\n      <td>1.868300</td>\n    </tr>\n    <tr>\n      <td>53700</td>\n      <td>1.822200</td>\n    </tr>\n    <tr>\n      <td>53800</td>\n      <td>1.861800</td>\n    </tr>\n    <tr>\n      <td>53900</td>\n      <td>1.857500</td>\n    </tr>\n    <tr>\n      <td>54000</td>\n      <td>1.893200</td>\n    </tr>\n    <tr>\n      <td>54100</td>\n      <td>1.869100</td>\n    </tr>\n    <tr>\n      <td>54200</td>\n      <td>1.872100</td>\n    </tr>\n    <tr>\n      <td>54300</td>\n      <td>1.823500</td>\n    </tr>\n    <tr>\n      <td>54400</td>\n      <td>1.910100</td>\n    </tr>\n    <tr>\n      <td>54500</td>\n      <td>1.909300</td>\n    </tr>\n    <tr>\n      <td>54600</td>\n      <td>1.852400</td>\n    </tr>\n    <tr>\n      <td>54700</td>\n      <td>1.805000</td>\n    </tr>\n    <tr>\n      <td>54800</td>\n      <td>1.901000</td>\n    </tr>\n    <tr>\n      <td>54900</td>\n      <td>1.840400</td>\n    </tr>\n    <tr>\n      <td>55000</td>\n      <td>1.850000</td>\n    </tr>\n    <tr>\n      <td>55100</td>\n      <td>1.878800</td>\n    </tr>\n    <tr>\n      <td>55200</td>\n      <td>1.914400</td>\n    </tr>\n    <tr>\n      <td>55300</td>\n      <td>1.863500</td>\n    </tr>\n    <tr>\n      <td>55400</td>\n      <td>1.897300</td>\n    </tr>\n    <tr>\n      <td>55500</td>\n      <td>1.916700</td>\n    </tr>\n    <tr>\n      <td>55600</td>\n      <td>1.850400</td>\n    </tr>\n    <tr>\n      <td>55700</td>\n      <td>1.882300</td>\n    </tr>\n    <tr>\n      <td>55800</td>\n      <td>1.836900</td>\n    </tr>\n    <tr>\n      <td>55900</td>\n      <td>1.888900</td>\n    </tr>\n    <tr>\n      <td>56000</td>\n      <td>1.883600</td>\n    </tr>\n    <tr>\n      <td>56100</td>\n      <td>1.955800</td>\n    </tr>\n    <tr>\n      <td>56200</td>\n      <td>1.912300</td>\n    </tr>\n    <tr>\n      <td>56300</td>\n      <td>1.890500</td>\n    </tr>\n    <tr>\n      <td>56400</td>\n      <td>1.876000</td>\n    </tr>\n    <tr>\n      <td>56500</td>\n      <td>1.898000</td>\n    </tr>\n    <tr>\n      <td>56600</td>\n      <td>1.858900</td>\n    </tr>\n    <tr>\n      <td>56700</td>\n      <td>1.834000</td>\n    </tr>\n    <tr>\n      <td>56800</td>\n      <td>1.933700</td>\n    </tr>\n    <tr>\n      <td>56900</td>\n      <td>1.867200</td>\n    </tr>\n    <tr>\n      <td>57000</td>\n      <td>1.837400</td>\n    </tr>\n    <tr>\n      <td>57100</td>\n      <td>1.835700</td>\n    </tr>\n    <tr>\n      <td>57200</td>\n      <td>1.933800</td>\n    </tr>\n    <tr>\n      <td>57300</td>\n      <td>1.866900</td>\n    </tr>\n    <tr>\n      <td>57400</td>\n      <td>1.830200</td>\n    </tr>\n    <tr>\n      <td>57500</td>\n      <td>1.848700</td>\n    </tr>\n    <tr>\n      <td>57600</td>\n      <td>1.934800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"stream","name":"stdin","text":"Enter an image URL for caption prediction:  https://static.vecteezy.com/system/resources/thumbnails/036/324/708/small/ai-generated-picture-of-a-tiger-walking-in-the-forest-photo.jpg\n"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9b9d36a93df3>\u001b[0m in \u001b[0;36m<cell line: 232>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;31m# Example inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0mtest_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter an image URL for caption prediction: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m \u001b[0mpredicted_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_caption_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Caption:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_caption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-9b9d36a93df3>\u001b[0m in \u001b[0;36mpredict_caption_from_url\u001b[0;34m(url, model, feature_extractor, tokenizer, device, max_length, num_beams)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3534\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3535\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3536\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7b987ccba660>"],"ename":"UnidentifiedImageError","evalue":"cannot identify image file <_io.BytesIO object at 0x7b987ccba660>","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"test_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(test_url, model, feature_extractor, tokenizer, device)\nprint(\"Predicted Caption:\", predicted_caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:15:44.228579Z","iopub.execute_input":"2025-04-04T04:15:44.228940Z","iopub.status.idle":"2025-04-04T04:15:46.950716Z","shell.execute_reply.started":"2025-04-04T04:15:44.228910Z","shell.execute_reply":"2025-04-04T04:15:46.949850Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter an image URL for caption prediction:  https://images.pexels.com/photos/1054655/pexels-photo-1054655.jpeg?cs=srgb&dl=pexels-hsapir-1054655.jpg&fm=jpg\n"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1527: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Predicted Caption: A silhouette of a person in a body of water . The sun is casting a shadow on the ground . Another silhouette is in the distance .\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"test_url = input(\"Enter an image URL for caption prediction: \").strip()\npredicted_caption = predict_caption_from_url(test_url, model, feature_extractor, tokenizer, device)\nprint(\"Predicted Caption:\", predicted_caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:16:20.145004Z","iopub.execute_input":"2025-04-04T04:16:20.145405Z","iopub.status.idle":"2025-04-04T04:16:22.524262Z","shell.execute_reply.started":"2025-04-04T04:16:20.145370Z","shell.execute_reply":"2025-04-04T04:16:22.523577Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter an image URL for caption prediction:  https://iso.500px.com/wp-content/uploads/2018/05/Blog-marketplace-getty500px-48429366-nologo-3000x2000.png\n"},{"name":"stdout","text":"Predicted Caption: A man in a red shirt is standing in front of a large green field of green grass . Another man is in the background . Sooners is taking a\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Flickr30k","metadata":{}},{"cell_type":"code","source":"!pip install --quiet evaluate nltk\n!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:28:54.974761Z","iopub.execute_input":"2025-04-27T19:28:54.975185Z","iopub.status.idle":"2025-04-27T19:29:04.703684Z","shell.execute_reply.started":"2025-04-27T19:28:54.975146Z","shell.execute_reply":"2025-04-27T19:29:04.702867Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=abc27734835cb179ea32dcd737cbc3dda8791a6869a691bf2bb86dda7e34c2ce\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install --quiet git+https://github.com/salaniz/pycocoevalcap.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:29:04.705027Z","iopub.execute_input":"2025-04-27T19:29:04.705366Z","iopub.status.idle":"2025-04-27T19:29:20.579164Z","shell.execute_reply.started":"2025-04-27T19:29:04.705332Z","shell.execute_reply":"2025-04-27T19:29:20.578312Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# # # 0. Clean Up & Install (Optional)\n# #####################################\n# # If you installed \"peft\" or older versions that conflict, remove them:\n# # !pip uninstall -y peft transformers\n# # Reinstall Transformers 4.31.0 (or similar) + Accelerate\n# # !pip install --no-cache-dir transformers==4.31.0 accelerate\n\n# #####################################\n# # 1. Imports and Setup\n# #####################################\n# import os\n# import ssl\n# import requests\n# import pandas as pd\n# import torch\n# from torch.utils.data import Dataset, DataLoader\n# from torchvision import transforms\n# from PIL import Image\n# from io import BytesIO\n\n# # Optional if you run into SSL issues\n# ssl._create_default_https_context = ssl._create_unverified_context\n\n# # Hugging Face Transformers\n# from transformers import (\n#     VisionEncoderDecoderModel,\n#     ViTImageProcessor,\n#     AutoTokenizer,\n#     Seq2SeqTrainingArguments,\n#     Seq2SeqTrainer,\n#     default_data_collator,\n#     set_seed,\n# )\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# set_seed(42)\n\n# #####################################\n# # 2. Config & Paths\n# #####################################\n# csv_file = \"/kaggle/input/flickr30k/captions.txt\"\n# img_dir  = \"/kaggle/input/flickr30k/flickr30k_images\"\n\n# encoder_model_name = \"google/vit-base-patch16-224-in21k\"\n# decoder_model_name = \"gpt2\"\n\n# # Updated hyperparameters\n# EPOCHS              = 3\n# BATCH_SIZE          = 2  # reduce from 8 to 2\n# LEARNING_RATE       = 5e-5\n# MAX_SEQ_LEN         = 32\n# FREEZE_ENCODER      = True   # freeze ViT\n# USE_GRAD_CHECKPOINT = False  # set True if you need more memory savings\n# USE_FP16            = True   # half precision -> significantly lowers memory usage\n\n# #####################################\n# # 3. Load the Captions CSV\n# #####################################\n# df = pd.read_csv(csv_file)\n# print(\"Number of captions:\", len(df))\n# print(df.head())\n\n# #####################################\n# # 4. Image Processor & Tokenizer\n# #####################################\n# feature_extractor = ViTImageProcessor.from_pretrained(encoder_model_name)\n# tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n# if tokenizer.pad_token is None:\n#     tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n# #####################################\n# # 5. Dataset\n# #####################################\n# class ImageCaptionDataset(Dataset):\n#     def __init__(self, dataframe, img_dir, feature_extractor, tokenizer, max_target_length=32, transforms=None):\n#         self.df = dataframe.reset_index(drop=True)\n#         self.img_dir = img_dir\n#         self.feature_extractor = feature_extractor\n#         self.tokenizer = tokenizer\n#         self.max_target_length = max_target_length\n#         self.transforms = transforms\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx):\n#         row = self.df.iloc[idx]\n#         img_name = row[\"image_name\"]\n#         caption  = str(row[\"comment\"])\n\n#         # load image\n#         path = os.path.join(self.img_dir, img_name)\n#         with Image.open(path).convert(\"RGB\") as image:\n#             if self.transforms is not None:\n#                 image = self.transforms(image)\n\n#         pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values.squeeze()\n\n#         # tokenize caption\n#         labels = self.tokenizer(\n#             caption,\n#             padding=\"max_length\",\n#             truncation=True,\n#             max_length=self.max_target_length\n#         ).input_ids\n\n#         # replace pad_token_id with -100\n#         labels = [(lbl if lbl != self.tokenizer.pad_token_id else -100) for lbl in labels]\n        \n#         return {\n#             \"pixel_values\": pixel_values,\n#             \"labels\": torch.tensor(labels, dtype=torch.long),\n#         }\n\n# # Basic transform: resize to 224x224\n# train_transforms = transforms.Compose([\n#     transforms.Resize((224, 224)),\n# ])\n\n# # create dataset\n# dataset = ImageCaptionDataset(\n#     df,\n#     img_dir,\n#     feature_extractor=feature_extractor,\n#     tokenizer=tokenizer,\n#     max_target_length=MAX_SEQ_LEN,\n#     transforms=train_transforms\n# )\n\n# # small train-val split\n# train_size = int(0.95 * len(dataset))\n# val_size   = len(dataset) - train_size\n# train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n# print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n\n# #####################################\n# # 6. ViT + GPT-2 Model\n# #####################################\n# model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n#     encoder_model_name, \n#     decoder_model_name\n# )\n# model.config.decoder_start_token_id = tokenizer.bos_token_id\n# model.config.eos_token_id           = tokenizer.eos_token_id\n# model.config.pad_token_id           = tokenizer.pad_token_id\n\n# # Generation settings\n# model.config.max_length           = MAX_SEQ_LEN\n# model.config.early_stopping       = True\n# model.config.no_repeat_ngram_size = 2\n\n# # (Optional) Freeze the entire ViT encoder to save memory\n# if FREEZE_ENCODER:\n#     for param in model.encoder.parameters():\n#         param.requires_grad = False\n\n# # (Optional) Gradient checkpointing\n# model.decoder.gradient_checkpointing = USE_GRAD_CHECKPOINT\n\n# model.to(device)\n\n# #####################################\n# # 7. Seq2SeqTrainer Setup\n# #####################################\n# def collate_fn(batch):\n#     pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n#     labels       = torch.stack([item[\"labels\"] for item in batch])\n#     return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n# training_args = Seq2SeqTrainingArguments(\n#     output_dir=\"vit-gpt2-checkpoints\",\n#     per_device_train_batch_size=BATCH_SIZE,\n#     per_device_eval_batch_size=BATCH_SIZE,\n#     # Turn off evaluation to prevent OOM mid-training\n#     evaluation_strategy=\"no\",\n#     # Turn off checkpoint saving to reduce overhead\n#     save_strategy=\"no\",\n#     num_train_epochs=EPOCHS,\n#     learning_rate=LEARNING_RATE,\n#     logging_steps=100,\n#     report_to=\"none\",\n#     push_to_hub=False,\n#     fp16=USE_FP16,                        # mixed precision\n#     gradient_checkpointing=USE_GRAD_CHECKPOINT,  # or keep the code here as well\n# )\n\n# # minimal dummy metric\n# def compute_metrics(eval_pred):\n#     return {\"dummy_metric\": 0.0}\n\n# trainer = Seq2SeqTrainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_ds,\n#     eval_dataset=val_ds,  # won't run since evaluation_strategy=\"no\"\n#     data_collator=collate_fn,\n#     tokenizer=feature_extractor,\n#     compute_metrics=compute_metrics,\n# )\n\n# #####################################\n# # 8. Train\n# #####################################\n# trainer.train()\n\n# #####################################\n# # 9.  Quality metrics (BLEU‑4, ROUGE‑L, CIDEr)\n# #####################################\n# import evaluate\n# import matplotlib.pyplot as plt\n# import tqdm\n# import nltk\n# from torch.utils.data import DataLoader\n\n# # 1) load BLEU & ROUGE\n# bleu_metric  = evaluate.load(\"bleu\")\n# rouge_metric = evaluate.load(\"rouge\")\n\n# # 2) load or fallback CIDER\n# try:\n#     cider_metric = evaluate.load(\"cider\")\n#     use_hf_cider = True\n# except Exception:\n#     from pycocoevalcap.cider.cider import Cider\n#     cider_scorer  = Cider()\n#     use_hf_cider  = False\n\n# # 3) helper to generate preds & refs\n# def generate_captions(dataloader):\n#     preds, refs = [], []\n#     model.eval()\n#     with torch.no_grad():\n#         for batch in tqdm.tqdm(dataloader, desc=\"Generating captions\"):\n#             pv        = batch[\"pixel_values\"].to(device)\n#             labels_id = batch[\"labels\"]\n\n#             # collect references\n#             for ids in labels_id:\n#                 seq = [i for i in ids.tolist() if i != -100]\n#                 refs.append(tokenizer.decode(seq, skip_special_tokens=True))\n\n#             # generate\n#             gen_ids = model.generate(\n#                 pv,\n#                 max_length=MAX_SEQ_LEN,\n#                 num_beams=4,\n#                 eos_token_id=tokenizer.eos_token_id,\n#                 pad_token_id=tokenizer.pad_token_id,\n#             )\n#             preds.extend(tokenizer.batch_decode(gen_ids, skip_special_tokens=True))\n#     return preds, refs\n\n# # 4) DataLoader for val split\n# val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n# preds, refs = generate_captions(val_loader)\n\n# # 5) compute BLEU‑4\n# bleu_score = bleu_metric.compute(\n#     predictions=[nltk.word_tokenize(p.lower()) for p in preds],\n#     references=[[nltk.word_tokenize(r.lower())] for r in refs],\n# )[\"bleu\"]\n\n# # 6) compute ROUGE‑L\n# rouge_score = rouge_metric.compute(predictions=preds, references=refs)[\"rougeL\"]\n\n# # 7) compute CIDEr\n# if use_hf_cider:\n#     cider_score = cider_metric.compute(predictions=preds, references=refs)[\"cider\"]\n# else:\n#     gts = {i: [refs[i]] for i in range(len(refs))}\n#     res = {i: [preds[i]] for i in range(len(preds))}\n#     cider_score, _ = cider_scorer.compute_score(gts, res)\n\n# # 8) print & store\n# print(f\"\\nBLEU‑4  : {bleu_score:.4f}\")\n# print(f\"ROUGE‑L : {rouge_score:.4f}\")\n# print(f\"CIDEr   : {cider_score:.4f}\")\n\n# # variables you can reuse downstream\n# _bleu_score  = bleu_score\n# _rouge_score = rouge_score\n# _cider_score = cider_score\n\n# # 9) simple bar chart\n# scores = {\"BLEU‑4\": bleu_score, \"ROUGE‑L\": rouge_score, \"CIDEr\": cider_score}\n# plt.figure(figsize=(6,4))\n# plt.bar(scores.keys(), scores.values())\n# plt.ylim(0, max(scores.values())*1.1)\n# plt.title(\"Validation Caption Quality\")\n# plt.ylabel(\"Score\")\n# plt.show()\n\n# #####################################\n# # 10. Inference from a URL\n# #####################################\n# def predict_caption_from_url(url, model, feature_extractor, tokenizer, device,\n#                              max_length=32, num_beams=4):\n#     model.eval()\n#     response = requests.get(url)\n#     image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n#     pixel_values = feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n#     with torch.no_grad():\n#         output_ids = model.generate(\n#             pixel_values,\n#             max_length=max_length,\n#             num_beams=num_beams,\n#             eos_token_id=tokenizer.eos_token_id,\n#             pad_token_id=tokenizer.pad_token_id\n#         )\n#     return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n# # Example usage:\n# test_url = input(\"Enter an image URL for caption prediction: \").strip()\n# print(\"Predicted Caption:\", predict_caption_from_url(\n#     test_url, model, feature_extractor, tokenizer, device,\n#     max_length=MAX_SEQ_LEN, num_beams=4\n# ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:33:47.821058Z","iopub.execute_input":"2025-04-27T19:33:47.821419Z","iopub.status.idle":"2025-04-27T19:33:47.828179Z","shell.execute_reply.started":"2025-04-27T19:33:47.821392Z","shell.execute_reply":"2025-04-27T19:33:47.827209Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ==============================================================\n# 0. (Optional) clean-up + installs\n# --------------------------------------------------------------\n# !pip uninstall -y peft transformers\n# !pip install --no-cache-dir transformers==4.31.0 accelerate evaluate nltk matplotlib\n\n# ==============================================================\n# 1. imports & setup\n# --------------------------------------------------------------\nimport os, ssl, requests, tqdm, nltk, evaluate, torch, pandas as pd\nfrom PIL import Image\nfrom io  import BytesIO\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import (\n    VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer,\n    Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainerCallback, set_seed\n)\n\nssl._create_default_https_context = ssl._create_unverified_context\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nset_seed(42)\nnltk.download(\"punkt\", quiet=True)\n\n# ==============================================================\n# 2. paths & h-params\n# --------------------------------------------------------------\ncsv_file, img_dir = \"/kaggle/input/flickr30k/captions.txt\", \"/kaggle/input/flickr30k/flickr30k_images\"\n\nencoder_model_name, decoder_model_name = \"google/vit-base-patch16-224-in21k\", \"gpt2\"\n\nEPOCHS, BATCH_SIZE      = 3, 2\nLR, MAX_SEQ_LEN         = 5e-5, 32\nFREEZE_ENCODER          = True\nUSE_GRAD_CHECKPOINT, FP16 = False, True\n\n# ==============================================================\n# 3. data\n# --------------------------------------------------------------\ndf = pd.read_csv(csv_file)\nfeature_extractor = ViTImageProcessor.from_pretrained(encoder_model_name)\ntokenizer         = AutoTokenizer.from_pretrained(decoder_model_name)\ntokenizer.pad_token, tokenizer.padding_side = tokenizer.eos_token, \"right\"\n\nclass FlickrDataset(Dataset):\n    def __init__(self, frame, img_root, fe, tok, max_len=32, tfm=None):\n        self.df, self.img_root, self.fe, self.tok, self.max_len, self.tfm = (\n            frame.reset_index(drop=True), img_root, fe, tok, max_len, tfm\n        )\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row  = self.df.iloc[idx]\n        img  = Image.open(os.path.join(self.img_root, row[\"image_name\"])).convert(\"RGB\")\n        if self.tfm: img = self.tfm(img)\n        px    = self.fe(img, return_tensors=\"pt\").pixel_values.squeeze()\n        lbls  = self.tok(\n            str(row[\"comment\"]), truncation=True, padding=\"max_length\", max_length=self.max_len\n        ).input_ids\n        lbls  = [(x if x != self.tok.pad_token_id else -100) for x in lbls]\n        return {\"pixel_values\": px, \"labels\": torch.tensor(lbls)}\n\nresize   = transforms.Compose([transforms.Resize((224,224))])\nfull_ds  = FlickrDataset(df, img_dir, feature_extractor, tokenizer, MAX_SEQ_LEN, resize)\ntrain_ds, val_ds = random_split(full_ds, [int(0.95*len(full_ds)), len(full_ds)-int(0.95*len(full_ds))])\n\ndef collate(batch):\n    return {\n        \"pixel_values\": torch.stack([b[\"pixel_values\"] for b in batch]),\n        \"labels\"      : torch.stack([b[\"labels\"]       for b in batch])\n    }\n\n# ==============================================================\n# 4. model\n# --------------------------------------------------------------\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n    encoder_model_name, decoder_model_name\n)\n# --- FIX for the TypeError: set attrs directly -------------\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.eos_token_id           = tokenizer.eos_token_id\nmodel.config.pad_token_id           = tokenizer.pad_token_id\n# ------------------------------------------------------------\nmodel.config.max_length, model.config.no_repeat_ngram_size, model.config.early_stopping = (\n    MAX_SEQ_LEN, 2, True\n)\nif FREEZE_ENCODER:\n    for p in model.encoder.parameters(): p.requires_grad = False\nmodel.decoder.gradient_checkpointing = USE_GRAD_CHECKPOINT\nmodel.to(device)\n\n# ==============================================================\n# 5. metrics helpers\n# --------------------------------------------------------------\nbleu_metric  = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\ntry:\n    cider_metric, USE_HF_CIDER = evaluate.load(\"cider\"), True\nexcept Exception:\n    from pycocoevalcap.cider.cider import Cider\n    cider_metric, USE_HF_CIDER = Cider(), False\n\ndef _compute_caption_metrics(pred_txt, ref_txt):\n    bleu_dict = bleu_metric.compute(\n        predictions=[nltk.word_tokenize(p.lower()) for p in pred_txt],\n        references=[[nltk.word_tokenize(r.lower())] for r in ref_txt]\n    )\n    precision  = bleu_dict[\"precisions\"][0]        # unigram precision\n    bleu       = bleu_dict[\"bleu\"]\n    rouge      = rouge_metric.compute(predictions=pred_txt, references=ref_txt)[\"rougeL\"]\n    if USE_HF_CIDER:\n        cider   = cider_metric.compute(predictions=pred_txt, references=ref_txt)[\"cider\"]\n    else:\n        gts  = {i:[ref_txt[i]] for i in range(len(ref_txt))}\n        res  = {i:[pred_txt[i]] for i in range(len(pred_txt))}\n        cider, _ = cider_metric.compute_score(gts, res)\n    return {\"precision\": precision, \"bleu\": bleu, \"rougeL\": rouge, \"cider\": cider}\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    pred_txt = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    ref_txt  = []\n    for lbl in labels:\n        ids = [i for i in lbl if i != -100 and i != tokenizer.pad_token_id]\n        ref_txt.append(tokenizer.decode(ids, skip_special_tokens=True))\n    return _compute_caption_metrics(pred_txt, ref_txt)\n\n# ==============================================================\n# 6. custom callback to print / update the table\n# --------------------------------------------------------------\nclass LiveTableCallback(TrainerCallback):\n    def __init__(self): self.rows, self.last_train_loss = [], None\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        # capture running train loss so we can print it when eval ends\n        if logs and \"loss\" in logs and \"epoch\" in logs: \n            self.last_train_loss = logs[\"loss\"]\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        ep = int(state.epoch)\n        self.rows.append({\n            \"epoch\"      : ep,\n            \"train_loss\" : self.last_train_loss,\n            \"val_loss\"   : metrics.get(\"eval_loss\"),\n            \"precision\"  : metrics.get(\"eval_precision\"),\n            \"BLEU-4\"     : metrics.get(\"eval_bleu\"),\n            \"ROUGE-L\"    : metrics.get(\"eval_rougeL\"),\n            \"CIDEr\"      : metrics.get(\"eval_cider\")\n        })\n        df = pd.DataFrame(self.rows)\n        print(\"\\n######### Epoch summary #########\")\n        print(df.to_string(index=False, float_format=\"%.4f\"))\n        print(\"#################################\\n\")\n\ntable_cb = LiveTableCallback()\n\n# ==============================================================\n# 7. trainer\n# --------------------------------------------------------------\nargs = Seq2SeqTrainingArguments(\n    output_dir                   = \"vit-gpt2-checkpoints\",\n    num_train_epochs             = EPOCHS,\n    learning_rate                = LR,\n    per_device_train_batch_size  = BATCH_SIZE,\n    per_device_eval_batch_size   = BATCH_SIZE,\n    logging_steps                = 100,          # <- keep your 100-step logs\n    logging_strategy             = \"steps\",\n    evaluation_strategy          = \"epoch\",      # <- add val pass every epoch\n    predict_with_generate        = True,\n    generation_max_length        = MAX_SEQ_LEN,\n    generation_num_beams         = 4,\n    save_strategy                = \"no\",\n    report_to                    = \"none\",\n    fp16                         = FP16,\n    gradient_checkpointing       = USE_GRAD_CHECKPOINT\n)\n\ntrainer = Seq2SeqTrainer(\n    model           = model,\n    args            = args,\n    train_dataset   = train_ds,\n    eval_dataset    = val_ds,\n    data_collator   = collate,\n    tokenizer       = tokenizer,          # IMPORTANT: text tokenizer\n    compute_metrics = compute_metrics,\n    callbacks       = [table_cb],         # live table printer\n)\n\n# ==============================================================\n# 8. train\n# --------------------------------------------------------------\ntrainer.train()\n\n# (The table is printed automatically after each epoch)\n# ==============================================================\n# 9. inference helper (unchanged)\n# --------------------------------------------------------------\ndef caption_from_url(url, model=model, fe=feature_extractor, tok=tokenizer,\n                     device=device, max_len=32, num_beams=4):\n    model.eval()\n    img = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\n    px  = fe(img, return_tensors=\"pt\").pixel_values.to(device)\n    with torch.no_grad():\n        out_ids = model.generate(px, max_length=max_len, num_beams=num_beams,\n                                 eos_token_id=tok.eos_token_id, pad_token_id=tok.pad_token_id)\n    return tok.decode(out_ids[0], skip_special_tokens=True)\n\n# Example:\n# url = \"https://farm3.staticflickr.com/2029/2212815924_3f6a805ec7_z.jpg\"\n# print(caption_from_url(url))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:37:31.723749Z","iopub.execute_input":"2025-04-27T19:37:31.724074Z","iopub.status.idle":"2025-04-27T22:45:10.247982Z","shell.execute_reply.started":"2025-04-27T19:37:31.724045Z","shell.execute_reply":"2025-04-27T22:45:10.246528Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-10-f89ca42553ed>:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75486' max='226455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 75486/226455 2:20:52 < 4:41:44, 8.93 it/s, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='3973' max='3973' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3973/3973 46:36]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1527: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-f89ca42553ed>\u001b[0m in \u001b[0;36m<cell line: 189>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;31m# 8. train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;31m# (The table is printed automatically after each epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2615\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2616\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2618\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3045\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3048\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     def predict(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4050\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4051\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4052\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4053\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4338\u001b[0m             \u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"losses\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_losses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4339\u001b[0m             \u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_inputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"inputs\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4340\u001b[0;31m             metrics = self.compute_metrics(\n\u001b[0m\u001b[1;32m   4341\u001b[0m                 \u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4342\u001b[0m             )\n","\u001b[0;32m<ipython-input-10-f89ca42553ed>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mref_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compute_caption_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# ==============================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-f89ca42553ed>\u001b[0m in \u001b[0;36m_compute_caption_metrics\u001b[0;34m(pred_txt, ref_txt)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_compute_caption_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     bleu_dict = bleu_metric.compute(\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_txt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mref_txt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_feature_from_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m_infer_feature_from_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_feature_from_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_infer_feature_from_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m_infer_feature_from_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;34mf\"Input references: {summarize_if_long_list(example['references'])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['a', 'man', 'and', ..., '.', 'and', 'the'],\nInput references: [['a', 'group', 'of', 'people', 'in', 'formal', 'attire', '.']]"],"ename":"ValueError","evalue":"Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['a', 'man', 'and', ..., '.', 'and', 'the'],\nInput references: [['a', 'group', 'of', 'people', 'in', 'formal', 'attire', '.']]","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# 1. Choose a directory to store your model\noutput_dir = \"vit_gpt2_flickr30k__model\"\n\n# 2. Save the model weights & config\nmodel.save_pretrained(output_dir)\n\n# 3. Save the feature extractor and tokenizer\nfeature_extractor.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model, feature_extractor and tokenizer saved to ./{output_dir}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T23:15:38.865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}